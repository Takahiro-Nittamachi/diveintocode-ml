{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQUoOMIvrsen"
   },
   "source": [
    "# 1.この課題の目的\n",
    "- 自然言語処理を体験する\n",
    "- 簡単な文書の分析ができるようになる\n",
    "以下の要件をすべて満たしていた場合、合格とします。\n",
    "\n",
    "※Jupyter Notebookを使い課題に沿った検証や説明ができている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q5bGMiNwrseo"
   },
   "source": [
    "# 【問題1】BoWとN-gram(手計算)\n",
    "目的\n",
    "\n",
    "- 古典的かつ強力な手法BoWとN-gramの理解\n",
    "以下は俳優K.Kさんのつぶやき(コーパス)です。\n",
    ">文1: 今撮影中で〜す！\n",
    ">\n",
    ">文2: 今休憩中で〜す(^^)\n",
    ">\n",
    ">文3: 今日ドラマ撮影で〜す！\n",
    ">    \n",
    ">文4: 今日、映画瞬公開で〜す！！！\n",
    "\n",
    "## 【問】\n",
    "特殊文字除去(!や〜など)、単語分割をし以下の2パターンで文1〜文4を数値化(ベクトル化)してください。\n",
    "\n",
    "- BoW(1-gram)\n",
    "- BoW(2-gram)\n",
    "\n",
    "手計算の後見やすい形にしてください。\n",
    "\n",
    "例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\takahiro\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import POSKeepFilter, CompoundNounFilter, TokenCountFilter\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import MeCab\n",
    "import math\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import os\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2057,
     "status": "ok",
     "timestamp": 1563159329983,
     "user": {
      "displayName": "新田町隆弘",
      "photoUrl": "",
      "userId": "01982770734455770299"
     },
     "user_tz": -540
    },
    "id": "ixwbjCLbrsep",
    "outputId": "45d95274-7c5a-40ff-c663-44cd73502cf5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>love</th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>the</th>\n",
       "      <th>baseball</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love baseball !!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I love this !</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    I  love  this  is  the  baseball\n",
       "I love baseball !!  1     1     0   0    0         1\n",
       "I love this !       1     1     1   0    0         0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = [\"I\", \"love\", \"this\", \"is\", \"the\", \"baseball\"]\n",
    "ms_kk_texts = [\"I love baseball !!\", \"I love this !\"]\n",
    "texts_vec = [[1,1,0,0,0,1], [1,1,1,0,0,0]]\n",
    "\n",
    "df_bow_1gram = pd.DataFrame(data = texts_vec, columns = vocabulary, index = ms_kk_texts)\n",
    "df_bow_1gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUEQu4Bfrses"
   },
   "source": [
    "## BoW(1-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2535,
     "status": "ok",
     "timestamp": 1563159330473,
     "user": {
      "displayName": "新田町隆弘",
      "photoUrl": "",
      "userId": "01982770734455770299"
     },
     "user_tz": -540
    },
    "id": "kfmY3IrArset",
    "outputId": "e7bc3fb2-7c33-497f-bf87-da7cc408d328"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>今</th>\n",
       "      <th>撮影</th>\n",
       "      <th>中</th>\n",
       "      <th>で〜す</th>\n",
       "      <th>休憩</th>\n",
       "      <th>今日</th>\n",
       "      <th>ドラマ</th>\n",
       "      <th>映画</th>\n",
       "      <th>瞬</th>\n",
       "      <th>公開</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>今撮影中で〜す！</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今休憩中で〜す(^^)</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今日ドラマ撮影で〜す！</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今日、映画瞬公開で〜す！！！</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                今  撮影  中  で〜す  休憩  今日  ドラマ  映画  瞬  公開\n",
       "今撮影中で〜す！        1   1  1    1   0   0    0   0  0   0\n",
       "今休憩中で〜す(^^)     1   0  1    1   1   0    0   0  0   0\n",
       "今日ドラマ撮影で〜す！     0   1  0    1   0   1    1   0  0   0\n",
       "今日、映画瞬公開で〜す！！！  0   0  0    1   0   1    0   1  1   1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = [\"今\", \"撮影\", \"中\", \"で〜す\", \"休憩\", \"今日\",\"ドラマ\", \"映画\", \"瞬\", \"公開\"]\n",
    "ms_kk_texts = [\"今撮影中で〜す！\", \"今休憩中で〜す(^^)\", \"今日ドラマ撮影で〜す！\",\\\n",
    "               \"今日、映画瞬公開で〜す！！！\"]\n",
    "texts_vec = [[1,1,1,1,0,0,0,0,0,0], [1,0,1,1,1,0,0,0,0,0],[0,1,0,1,0,1,1,0,0,0],\\\n",
    "             [0,0,0,1,0,1,0,1,1,1]]\n",
    "\n",
    "df_bow_1gram = pd.DataFrame(data = texts_vec, columns = vocabulary, index = ms_kk_texts)\n",
    "df_bow_1gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qW8klm06rsew"
   },
   "source": [
    "## BoW(2-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2525,
     "status": "ok",
     "timestamp": 1563159330474,
     "user": {
      "displayName": "新田町隆弘",
      "photoUrl": "",
      "userId": "01982770734455770299"
     },
     "user_tz": -540
    },
    "id": "h3vsvuVbrsew",
    "outputId": "4915eb23-f509-4b51-fb0c-1a5aec46d397"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>今撮影</th>\n",
       "      <th>撮影中</th>\n",
       "      <th>中で〜す</th>\n",
       "      <th>今休憩</th>\n",
       "      <th>休憩中</th>\n",
       "      <th>中で〜す</th>\n",
       "      <th>今日ドラマ</th>\n",
       "      <th>ドラマ撮影</th>\n",
       "      <th>撮影で〜す</th>\n",
       "      <th>今日映画</th>\n",
       "      <th>映画瞬間</th>\n",
       "      <th>瞬公開</th>\n",
       "      <th>公開で〜す</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>今撮影中で〜す！</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今休憩中で〜す(^^)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今日ドラマ撮影で〜す！</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>今日、映画瞬公開で〜す！！！</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                今撮影  撮影中  中で〜す  今休憩  休憩中  中で〜す  今日ドラマ  ドラマ撮影  撮影で〜す  今日映画  \\\n",
       "今撮影中で〜す！          1    0     1    0    0     0      0      0      0     0   \n",
       "今休憩中で〜す(^^)       0    0     0    1    0     1      0      0      0     0   \n",
       "今日ドラマ撮影で〜す！       0    0     0    0    0     0      1      0      1     0   \n",
       "今日、映画瞬公開で〜す！！！    0    0     0    0    0     0      0      0      0     1   \n",
       "\n",
       "                映画瞬間  瞬公開  公開で〜す  \n",
       "今撮影中で〜す！           0    0      0  \n",
       "今休憩中で〜す(^^)        0    0      0  \n",
       "今日ドラマ撮影で〜す！        0    0      0  \n",
       "今日、映画瞬公開で〜す！！！     0    1      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = [\"今撮影\", \"撮影中\", \"中で〜す\", \"今休憩\", \"休憩中\",\"中で〜す\", \"今日ドラマ\",\\\n",
    "              \"ドラマ撮影\", \"撮影で〜す\", \"今日映画\", \"映画瞬間\",\"瞬公開\", \"公開で〜す\"]\n",
    "\n",
    "\n",
    "texts_vec = [[1,0,1,0,0,0,0,0,0,0,0,0,0],[0,0,0,1,0,1,0,0,0,0,0,0,0],\\\n",
    "             [0,0,0,0,0,0,1,0,1,0,0,0,0],[0,0,0,0,0,0,0,0,0,1,0,1,1]]\n",
    "\n",
    "df_bow_1gram = pd.DataFrame(data = texts_vec, columns = vocabulary, index = ms_kk_texts)\n",
    "df_bow_1gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G1o7s5gtrsez"
   },
   "source": [
    "# 【問題2】TF-IDF(手計算)\n",
    "目的\n",
    "\n",
    "- 古典的かつ強力なTF-IDFの理解\n",
    "\n",
    "## 標準的なTF-IDFの公式 \n",
    "$Term Frequency$:\n",
    "$$\n",
    "tf(t,d) = \\frac{n_{t,d}}{\\sum_{s \\in d}n_{s,d}}\n",
    "$$\n",
    "\n",
    "$n_{t,d}$: 文書$d$内の単語$t$の出現回数\n",
    "\n",
    "$\\sum_{s \\in d}n_{s,d}$\n",
    " : 文書$d$の全単語の出現回数の和\n",
    "\n",
    "\n",
    "$Inverse Document Frequency$:\n",
    "\n",
    "$$\n",
    "idf(t) = \\log_2{\\frac{N}{df(t)}}\n",
    "$$\n",
    "\n",
    "$N$ : 全文書数\n",
    "\n",
    "$df(t)$ : 単語$t$が出現する文書数\n",
    "\n",
    "$TF-IDF$:\n",
    "$$\n",
    "tfidf(t, d) = tf(t, d) \\times idf(t)\n",
    "$$\n",
    "\n",
    "## 【問】\n",
    "問題1のコーパスを使って、文1〜文4をTFIDFで数値化(ベクトル化)してください。 \n",
    "問題1と同様、手計算の後見やすい形にしてください。\n",
    "\n",
    "### 正解例\n",
    "tfidf(今, 文書1) = 0.25 になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZHsqjrSrsez"
   },
   "outputs": [],
   "source": [
    "## TFIDF(今、文書1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5xEu4K2rse1"
   },
   "outputs": [],
   "source": [
    "text1 = ['今', '撮影', '中', 'で〜す']\n",
    "text2 = ['今' ,'休憩', '中', 'で〜す']\n",
    "text3 = ['今日' ,'ドラマ', '撮影', 'で〜す']\n",
    "text4 = ['今日' ,'映画', '瞬', '公開', 'で〜す']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2518,
     "status": "ok",
     "timestamp": 1563159330482,
     "user": {
      "displayName": "新田町隆弘",
      "photoUrl": "",
      "userId": "01982770734455770299"
     },
     "user_tz": -540
    },
    "id": "Gv7t8nb-rse3",
    "outputId": "fbabef77-0b54-41d6-f178-ba84b628c23e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['今', '撮影', '中', 'で〜す'],\n",
       " ['今', '休憩', '中', 'で〜す'],\n",
       " ['今日', 'ドラマ', '撮影', 'で〜す'],\n",
       " ['今日', '映画', '瞬', '公開', 'で〜す']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = [text1, text2, text3, text4]\n",
    "all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例　TF-IDF(撮影, 文書1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGcKRKvCrse6"
   },
   "source": [
    "## TFの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n1bWjhZgrse6"
   },
   "outputs": [],
   "source": [
    "def compute_tf(text, word):\n",
    "    count = 0\n",
    "    for t in text:\n",
    "        if word == t:\n",
    "            count += 1\n",
    "    return count / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tPXTp2j3rse8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF=0.25\n"
     ]
    }
   ],
   "source": [
    "compute_tf(text1, '今')\n",
    "print('TF={0:.2f}'.format(compute_tf(text1, '今')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IBKCwe2_rse-"
   },
   "source": [
    "## IDFの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KoGdWHF2rse_"
   },
   "outputs": [],
   "source": [
    "def compute_idf(all_text, word):\n",
    "    count = 0\n",
    "    for t in all_text:\n",
    "        if word in t:\n",
    "            count += 1\n",
    "    return np.log(len(all_text) / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ul6sXF7LrsfB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF=0.69\n"
     ]
    }
   ],
   "source": [
    "print('IDF={0:.2f}'.format(compute_idf(all_text, '今')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDFの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(text, all_text, word):\n",
    "    return compute_tf(text, word) * compute_idf(all_text, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF=0.17\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF={0:.2f}'.format(compute_tf_idf(text1, all_text, '今')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例　TF-IDF(休憩, 文書2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34657359027997264"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_tf_idf(text2, all_text, '休憩')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF=0.35\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF={0:.2f}'.format(compute_tf_idf(text2, all_text, '休憩')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例　TF-IDF(ドラマ, 文書3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34657359027997264"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_tf_idf(text3, all_text, 'ドラマ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF=0.35\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF={0:.2f}'.format(compute_tf_idf(text3, all_text, 'ドラマ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例　TF-IDF(映画, 文書4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2772588722239781"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_tf_idf(text4, all_text, '映画')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF=0.28\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF={0:.2f}'.format(compute_tf_idf(text4, all_text, '映画')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0z3W41JrsfD"
   },
   "source": [
    "# 【問題3】テキストクリーニング(プログラミング)\n",
    "## 目的\n",
    "- 実データ対応のためのテキストクリーニングの理解\n",
    "- 正規表現の理解\n",
    "\n",
    "実際のテキストデータは非常に汚いことが多いです。 \n",
    "以下は3/6(水)にnoroさんがSlackで発言した文章で、良い例です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQwTdzehrsfD"
   },
   "source": [
    "＃<!everyone> *【スペシャル特典】有償のRubyMineやPyCharmの `6ヶ月間100%OFFクーポン` をご希望者の方先着100名様に贈呈いたします！*\\n\\nこの度、RubyMineやPyCharmのメーカーであるJetBrains社へのクーポンコードの提供交渉が実り、100クーポンをいただくことができました。\\n\\n```\\nRubyMine\\n<https://www.jetbrains.com/ruby/>\\n\\nPyCharm\\n<https://www.jetbrains.com/pycharm/>\\n```\\n\\n「ご希望の方は、手を挙げて！」方式で、ご希望の方はこの投稿の手あげスタンプをクリックしてください。\\n\\n期限は、 *`2019年3月20日（水）22:00まで`* とさせていただきます。\\nふるってのご希望をお待ちしております！ :smile:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IH3d-xJnrsfE"
   },
   "source": [
    "# 【問】\n",
    "このテキストに以下の処理を施してください。\n",
    "\n",
    "- urlを削除\n",
    "- 【〇〇】を削除\n",
    "- 改行等の特殊文字を削除\n",
    "- 絵文字除去\n",
    "\n",
    "ここではしませんが、数字を文字列NUMBERに置き換える処理をよくします。\n",
    "\n",
    "正解例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YfPh7rFDrsfF"
   },
   "source": [
    "有償のRubyMineやPyCharmの6ヶ月間100%OFFクーポンをご希望者の方先着100名様に贈呈いたします！この度、RubyMineやPyCharmのメーカーであるJetBrains社へのクーポンコードの提供交渉が実り、100クーポンをいただくことができました。RubyMinePyCharm「ご希望の方は、手を挙げて！」方式で、ご希望の方はこの投稿の手あげスタンプをクリックしてください。期限は、2019年3月20日（水）22:00までとさせていただきます。ふるってのご希望をお待ちしております！\n",
    "\n",
    "正規表現のサンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2517,
     "status": "ok",
     "timestamp": 1563159330497,
     "user": {
      "displayName": "新田町隆弘",
      "photoUrl": "",
      "userId": "01982770734455770299"
     },
     "user_tz": -540
    },
    "id": "dwHiRR51rsfG",
    "outputId": "513df6de-24e6-487c-d054-703cf8985b07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'【スペシャル特典】有償のRubyMineやPyCharmの 6ヶ月間100%OFFクーポン をご希望者の方先着100名様に贈呈いたします'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正規表現操作のライブラリ\n",
    "import re\n",
    "# 対象テキストデータ\n",
    "text = '【スペシャル特典】有償のRubyMineやPyCharmの `6ヶ月間100%OFFクーポン` をご希望者の方先着100名様に贈呈いたします！*\\n\\n'\n",
    "# re.compileを使うと処理が早くなります\n",
    "BAD_SYMBOL = re.compile('[\\n*！`]+')\n",
    "\n",
    "# re.sub(r'[\\n*！`]+', '', text)でもできます\n",
    "text = re.sub(BAD_SYMBOL, '', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2RZfRiIrsfI"
   },
   "source": [
    "リアルタイムで正規表現を確認できるサイトです。\n",
    "\n",
    "[https://regex101.com/](https://regex101.com/)\n",
    "\n",
    "[https://regex-testdrive.com/ja/dotest](https://regex-testdrive.com/ja/)\n",
    "\n",
    "[re 正規表現操作](https://docs.python.org/ja/3/library/re.html)\n",
    "\n",
    "## Tips NLPのLinuxコマンド\n",
    "これまでpythonでファイルを読み込んで処理をしていましたが、\n",
    "簡単な作業においてはlinuxコマンドの方がメモリの使用料が半分以下だったりとパフォーマンスが良いです。\n",
    "\n",
    "例えばファイルの行数を数えたい場合、pythonでわざわざ書くのは面倒です。\n",
    "以下の1行のコマンドで実行できます。\n",
    "\n",
    ">wc -l 〇〇.txt\n",
    "\n",
    "また分割したい場合はsplit\n",
    "\n",
    "並び替えたい場合はsort\n",
    "\n",
    "置換にはsed\n",
    "\n",
    "文の先頭、後頭部分を見たければhead,tail\n",
    "\n",
    "など便利なコマンドがあります。\n",
    "\n",
    "詳しく知りたい方はNLP100本ノックで調べてみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2507,
     "status": "ok",
     "timestamp": 1563159330498,
     "user": {
      "displayName": "新田町隆弘",
      "photoUrl": "",
      "userId": "01982770734455770299"
     },
     "user_tz": -540
    },
    "id": "Fmb0FETJrsfJ",
    "outputId": "442c37d4-e62c-413f-f043-ba290cd60720"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'有償のRubyMineやPyCharmの6ヶ月間100%OFFクーポンをご希望者の方先着100名様に贈呈いたします！この度、RubyMineやPyCharmのメーカーであるJetBrains社へのクーポンコードの提供交渉が実り、100クーポンをいただくことができました。RubyMinePyCharm「ご希望の方は、手を挙げて！」方式で、ご希望の方はこの投稿の手あげスタンプをクリックしてください。期限は、2019年3月20日（水）22:00までとさせていただきます。ふるってのご希望をお待ちしております！'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '<!everyone> *【スペシャル特典】有償のRubyMineやPyCharmの `6ヶ月間100%OFFクーポン` をご希望者の方先着100名様に贈呈いたします！\\\n",
    "*\\n\\nこの度、RubyMineやPyCharmのメーカーであるJetBrains社へのクーポンコードの提供交渉が実り、100クーポンをいただくことができました。\\\n",
    "\\n\\n```\\nRubyMine\\n<https://www.jetbrains.com/ruby/>\\n\\nPyCharm\\n<https://www.jetbrains.com/pycharm/>\\n```\\n\\n\\\n",
    "「ご希望の方は、手を挙げて！」方式で、ご希望の方はこの投稿の手あげスタンプをクリックしてください。\\\n",
    "\\n\\n期限は、 *`2019年3月20日（水）22:00まで`* とさせていただきます。\\nふるってのご希望をお待ちしております！ :smile:'\n",
    "\n",
    "symbol_reg = r'[\\n*` ]+'\n",
    "mention_reg = r'<.*?>'\n",
    "phraze_reg = r'【.*?】'\n",
    "command_reg = r':[^0-9０-９]+:'\n",
    "\n",
    "reg_str = f'{symbol_reg}|{mention_reg}|{phraze_reg}|{command_reg}'\n",
    "reg_str = re.compile(reg_str)\n",
    "\n",
    "# re.sub(r'[\\n*！`]+', '', text)でもできます\n",
    "text = re.sub(reg_str, '', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTH08sCcrsfQ"
   },
   "source": [
    "# 【問題4】形態素解析\n",
    "目的\n",
    "\n",
    "- 形態素解析の理解\n",
    "形態素解析のツールはMecabやJanomeなど様々ですが、\n",
    "ここでは手軽に導入できるJanomeを使います。\n",
    "\n",
    "[Janome document](https://mocobeta.github.io/janome/)\n",
    "\n",
    "【問】\n",
    "上記のクリーニングしたテキストをJanomeを用いて形態素解析をし、\n",
    "名詞または動詞の単語を抜き出してください。\n",
    "\n",
    "正解例\n",
    ">[\"有償\", \"RubyMine\", \"Pycharm\", ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有償\t名詞,一般,*,*,*,*,有償,ユウショウ,ユーショー\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "RubyMine\t名詞,一般,*,*,*,*,RubyMine,*,*\n",
      "や\t助詞,並立助詞,*,*,*,*,や,ヤ,ヤ\n",
      "PyCharm\t名詞,一般,*,*,*,*,PyCharm,*,*\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "6\t名詞,数,*,*,*,*,6,*,*\n",
      "ヶ月\t名詞,接尾,助数詞,*,*,*,ヶ月,カゲツ,カゲツ\n",
      "間\t名詞,接尾,一般,*,*,*,間,カン,カン\n",
      "100\t名詞,数,*,*,*,*,100,*,*\n",
      "%\t名詞,サ変接続,*,*,*,*,%,*,*\n",
      "OFF\t名詞,一般,*,*,*,*,OFF,*,*\n",
      "クーポン\t名詞,一般,*,*,*,*,クーポン,クーポン,クーポン\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "ご\t接頭詞,名詞接続,*,*,*,*,ご,ゴ,ゴ\n",
      "希望\t名詞,サ変接続,*,*,*,*,希望,キボウ,キボー\n",
      "者\t名詞,接尾,一般,*,*,*,者,シャ,シャ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "方\t名詞,非自立,一般,*,*,*,方,ホウ,ホー\n",
      "先着\t名詞,サ変接続,*,*,*,*,先着,センチャク,センチャク\n",
      "100\t名詞,数,*,*,*,*,100,*,*\n",
      "名\t名詞,接尾,助数詞,*,*,*,名,メイ,メイ\n",
      "様\t名詞,接尾,人名,*,*,*,様,サマ,サマ\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "贈呈\t名詞,サ変接続,*,*,*,*,贈呈,ゾウテイ,ゾーテイ\n",
      "いたし\t動詞,非自立,*,*,五段・サ行,連用形,いたす,イタシ,イタシ\n",
      "ます\t助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n",
      "！\t記号,一般,*,*,*,*,！,！,！\n",
      "この\t連体詞,*,*,*,*,*,この,コノ,コノ\n",
      "度\t名詞,非自立,副詞可能,*,*,*,度,タビ,タビ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "RubyMine\t名詞,固有名詞,組織,*,*,*,RubyMine,*,*\n",
      "や\t助詞,並立助詞,*,*,*,*,や,ヤ,ヤ\n",
      "PyCharm\t名詞,一般,*,*,*,*,PyCharm,*,*\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "メーカー\t名詞,一般,*,*,*,*,メーカー,メーカー,メーカー\n",
      "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\n",
      "ある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n",
      "JetBrains\t名詞,一般,*,*,*,*,JetBrains,*,*\n",
      "社\t名詞,接尾,一般,*,*,*,社,シャ,シャ\n",
      "へ\t助詞,格助詞,一般,*,*,*,へ,ヘ,エ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "クーポン\t名詞,一般,*,*,*,*,クーポン,クーポン,クーポン\n",
      "コード\t名詞,一般,*,*,*,*,コード,コード,コード\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "提供\t名詞,サ変接続,*,*,*,*,提供,テイキョウ,テイキョー\n",
      "交渉\t名詞,サ変接続,*,*,*,*,交渉,コウショウ,コーショー\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "実り\t動詞,自立,*,*,五段・ラ行,連用形,実る,ミノリ,ミノリ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "100\t名詞,数,*,*,*,*,100,*,*\n",
      "クーポン\t名詞,一般,*,*,*,*,クーポン,クーポン,クーポン\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "いただく\t動詞,自立,*,*,五段・カ行イ音便,基本形,いただく,イタダク,イタダク\n",
      "こと\t名詞,非自立,一般,*,*,*,こと,コト,コト\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "でき\t動詞,自立,*,*,一段,連用形,できる,デキ,デキ\n",
      "まし\t助動詞,*,*,*,特殊・マス,連用形,ます,マシ,マシ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "RubyMinePyCharm\t名詞,一般,*,*,*,*,RubyMinePyCharm,*,*\n",
      "「\t記号,括弧開,*,*,*,*,「,「,「\n",
      "ご\t接頭詞,名詞接続,*,*,*,*,ご,ゴ,ゴ\n",
      "希望\t名詞,サ変接続,*,*,*,*,希望,キボウ,キボー\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "方\t名詞,非自立,一般,*,*,*,方,ホウ,ホー\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "手\t名詞,一般,*,*,*,*,手,テ,テ\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "挙げ\t動詞,自立,*,*,一段,連用形,挙げる,アゲ,アゲ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "！\t記号,一般,*,*,*,*,！,！,！\n",
      "」\t記号,括弧閉,*,*,*,*,」,」,」\n",
      "方式\t名詞,一般,*,*,*,*,方式,ホウシキ,ホーシキ\n",
      "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "ご\t接頭詞,名詞接続,*,*,*,*,ご,ゴ,ゴ\n",
      "希望\t名詞,サ変接続,*,*,*,*,希望,キボウ,キボー\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "方\t名詞,非自立,一般,*,*,*,方,ホウ,ホー\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "この\t連体詞,*,*,*,*,*,この,コノ,コノ\n",
      "投稿\t名詞,サ変接続,*,*,*,*,投稿,トウコウ,トーコー\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "手\t名詞,一般,*,*,*,*,手,テ,テ\n",
      "あげ\t動詞,自立,*,*,一段,連用形,あげる,アゲ,アゲ\n",
      "スタンプ\t名詞,一般,*,*,*,*,スタンプ,スタンプ,スタンプ\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "クリック\t名詞,一般,*,*,*,*,クリック,クリック,クリック\n",
      "し\t動詞,自立,*,*,サ変・スル,連用形,する,シ,シ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "ください\t動詞,非自立,*,*,五段・ラ行特殊,命令ｉ,くださる,クダサイ,クダサイ\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "期限\t名詞,一般,*,*,*,*,期限,キゲン,キゲン\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "、\t記号,読点,*,*,*,*,、,、,、\n",
      "2019\t名詞,数,*,*,*,*,2019,*,*\n",
      "年\t名詞,接尾,助数詞,*,*,*,年,ネン,ネン\n",
      "3\t名詞,数,*,*,*,*,3,*,*\n",
      "月\t名詞,一般,*,*,*,*,月,ツキ,ツキ\n",
      "20\t名詞,数,*,*,*,*,20,*,*\n",
      "日\t名詞,接尾,助数詞,*,*,*,日,ニチ,ニチ\n",
      "（\t記号,括弧開,*,*,*,*,（,（,（\n",
      "水\t名詞,一般,*,*,*,*,水,ミズ,ミズ\n",
      "）\t記号,括弧閉,*,*,*,*,）,）,）\n",
      "22\t名詞,数,*,*,*,*,22,*,*\n",
      ":\t名詞,サ変接続,*,*,*,*,:,*,*\n",
      "00\t名詞,数,*,*,*,*,00,*,*\n",
      "まで\t助詞,副助詞,*,*,*,*,まで,マデ,マデ\n",
      "と\t助詞,格助詞,引用,*,*,*,と,ト,ト\n",
      "さ\t動詞,自立,*,*,サ変・スル,未然レル接続,する,サ,サ\n",
      "せ\t動詞,接尾,*,*,一段,連用形,せる,セ,セ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "いただき\t動詞,非自立,*,*,五段・カ行イ音便,連用形,いただく,イタダキ,イタダキ\n",
      "ます\t助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "ふるって\t副詞,一般,*,*,*,*,ふるって,フルッテ,フルッテ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "ご\t接頭詞,名詞接続,*,*,*,*,ご,ゴ,ゴ\n",
      "希望\t名詞,サ変接続,*,*,*,*,希望,キボウ,キボー\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "お待ち\t名詞,サ変接続,*,*,*,*,お待ち,オマチ,オマチ\n",
      "し\t動詞,自立,*,*,サ変・スル,連用形,する,シ,シ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "おり\t動詞,非自立,*,*,五段・ラ行,連用形,おる,オリ,オリ\n",
      "ます\t助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n",
      "！\t記号,一般,*,*,*,*,！,！,！\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "for token in t.tokenize(text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有償\n",
      "RubyMine\n",
      "PyCharm\n",
      "OFF\n",
      "クーポン\n",
      "PyCharm\n",
      "メーカー\n",
      "JetBrains\n",
      "クーポン\n",
      "コード\n",
      "クーポン\n",
      "RubyMinePyCharm\n",
      "手\n",
      "方式\n",
      "手\n",
      "スタンプ\n",
      "クリック\n",
      "期限\n",
      "月\n",
      "水\n"
     ]
    }
   ],
   "source": [
    "for token in t.tokenize(text):\n",
    "    if token.part_of_speech.startswith('名詞,一般'):\n",
    "        print(token.surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39etQuMirsfd"
   },
   "source": [
    "# 【問題5】ニュースの分析\n",
    "目的\n",
    "\n",
    "- 日本語の自然言語処理の体験\n",
    "- 類似度の理解\n",
    "以下からldcc-20140209.tar.gzをダウンロードしてください。 \n",
    "\n",
    "[livedoor](https://www.rondhuit.com/download.html#ldcc)\n",
    "\n",
    "もしくはwgetコマンドを使っても良いです。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNFJ9xhGrsfd"
   },
   "outputs": [],
   "source": [
    "# # livedoorのnewsをダウンロード\n",
    "# wget 'https://www.rondhuit.com/download/ldcc-20140209.tar.gz'\n",
    "# # 圧縮ファイルを解凍\n",
    "# tar zxf ldcc-20140209.tar.gz\n",
    "# # livedoorニュースの説明を表示\n",
    "# cat text/README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CnmILoETrsfi"
   },
   "source": [
    "サンプルコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJcuKbrbrsfj"
   },
   "outputs": [],
   "source": [
    "# サブフォルダまで賢く読み込んでもらう\n",
    "from sklearn.datasets import load_files\n",
    "# encodingをutf-8指定して読み込み\n",
    "bin_data = load_files('./text', encoding='utf-8')\n",
    "documents = bin_data.data\n",
    "# 今回はラベルが無いと仮定してください\n",
    "# targets = bin_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RbriCsmSrsfk"
   },
   "source": [
    "【問】\n",
    "以下の流れでニュースを分析してください。\n",
    "\n",
    "- まずどんなニュースなのか読んでみる\n",
    "- 出現単語をカウントして分析する\n",
    "- テキストをクリーニングする\n",
    "- BoW + TFIDFでベクトル化する\n",
    "- あるニュースに一番cos類似度が近いニュースを出力する関数の作成\n",
    "- 別の類似度手法を1つ調べて上の関数に組み込む(切り替えられるようにする)\n",
    "- なぜそのような結果になったのか考察する\n",
    "\n",
    "[sklearn.feature_extraction.text](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AiM7Qc6lrsfl"
   },
   "source": [
    "## どんなニュースなのか？\n",
    "### 様々なジャンル(食事、芸能、IT、ゲーム)の出来事や話題となっている物などについて記載されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7376"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://news.livedoor.com/article/detail/4931238/\\n2010-08-08T10:00:00+0900\\nNY名物イベントが日本でも！名店グルメを気軽に楽しむ\\nニューヨークで20年続いている食の祭典「レストラン・ウィーク」。その日本版がダイナーズクラブ特別協賛のもと7月30日よりスタート。8月31日までの期間中、青山・六本木、丸の内、銀座、横浜のエリアから、ラグジュアリーレストラン81店舗がこのイベントのために特別用意したランチメニュー2010円（税・サ別）、ディナー5000円（税・サ別）を気軽に楽しめる、とっておきのイベントです。\\n\\u3000\\n\\u3000実行委員長には、学校法人服部学園、服部栄養専門学校 理事長・校長であり医学博士でもある服部幸應氏を迎え、実行委員に石田純一さん、LA BETTOLAオーナーシェフ落合務氏、フードアナリスト協会会長、高賀右近氏、つきぢ田村三代目、田村隆氏に、そして放送作家・脚本家の小山薫堂さんなど、食のスペシャリストたちが勢揃い。\\n\\n参加レストランには、ミシュランのフランス版、東京版ともに星を獲得している吉野建シェフの「レストラン タテル ヨシノ 汐留」や、日本料理の名門「つきぢ田村」、「金田中 庵」、「赤坂璃宮」に「mikuni MARUNOUCHI」など、日本を代表するレストランがずらり。\\n\\u3000イベント期間の〜8月19日までは、特別協賛のダイナーズクラブカード会員、またはシティバンクに口座を持つシティゴールドメンバーが楽しめる先行期間となりますが、その後は誰でも参加できるので、日程のチェックは必須。\\n\\n\\u3000予約方法は必ず事前に、各店舗に問合せを行い「ジャパンレストラン・ウィーク2010」での予約であることを伝えればOK！憧れていたレストランの料理をリーズナブルにいただけるチャンスです！極上の味とラグジュアリーな空間を満喫。そんな幸せを実感できる「ジャパンレストラン・ウィーク2010」にぜひ参加しててみてはいかがですか？\\n\\nJAPAN RESTAURANT WEEK 2010 -公式サイト\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://news.livedoor.com/article/detail/6655079/\\n2012-06-13T19:25:00+0900\\n小沢一郎氏の妻が支援者に離婚を報告。「週刊文春」報じる\\n13日、Web版「週刊文春」は、民主党の元代表・小沢一郎氏の妻が、支援者宛に離婚したことを伝える手紙を送ったと報じ、ツイッターやネット掲示板で大きな話題になっている。\\n\\n記事によると、その手紙は「小沢は放射能が怖くて秘書と一緒に逃げだしました」「隠し子が発覚した際、小沢元代表は和子夫人に謝るどころか、『いつでも離婚してやる』と言い放ち、和子夫人は一時は自殺まで考えた」という小沢氏の支持者にとってはショッキングな内容となっている。\\n\\nツイッターやネット掲示板では、「これが小沢一郎の本性か」「こりゃすごいな。てか、がっかりだなあ」などと、手紙の内容に驚く声が相次ぎ、その他にも「小沢潰しですかね?」「元秘書が交番で暴れて逮捕された件といい、小沢氏を潰したい何者かによる工作活動では?」といった声が挙がっていた。\\n\\n【関連情報】\\n・小沢一郎夫人が支援者に「離婚しました」（週刊文春）\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://news.livedoor.com/article/detail/5172424/\\n2010-11-30T08:00:00+0900\\n【Sports Watch】田中＆里田の交際、アプローチは里田から\\nグラビアアイドル・ほしのあき＆騎手・三浦皇成がお互いのブログで交際を認めた日、東北楽天ゴールデンイーグルスのエース・田中将大とタレント・里田まいの交際もまた公のものとなったが、本日30日（火）発売の「週刊アサヒ芸能」（12.9号）では、「マー君を“ナンパ”した里田」との見出しで、両者の交際にまつわる関係者の証言を紹介した。\\n\\n同誌にコメントを寄せた芸能デスクによると、「周囲の事情はさておき、当人たちが盛り上がっているのは確か。結婚も完全に視野に入れているようで、すでに新居を探しているとの話まである」という。\\n\\nまた、二人の交際は里田からのアプローチによるものとのことで、前出の芸能デスクは、「2人は昨年末の番組共演時に、里田から猛アタック。『一緒に食事でも』と誘い、春頃にはつきあい始めた。里田は仲のいいスザンヌ（24）、木下優樹菜（22）には、早くから相談していたし、周囲にも浮かれてしゃべりまくっていた。次世代エースをゲットしたわけですから、賞味期限切れ間近の里田にすれば、してやったりでしょう」とも語っている。\\n\\nちなみに、同じく同誌にコメントする球団関係者は、「高校時代は同じ高校の女子生徒と在学中の3年間、ずっとつきあっていましたね。基本はオクテ。プロ入り後にキャバクラなども覚えましたが、まぁ、モテるタイプではないし、女あしらいも不慣れ。高校時代はかなりワガママで、元カノは苦労が絶えなかったと聞いています。同年代の女より、里田のような姉さんタイプのほうが勝負の世界では向いているケースも多いですからね」と明かしており、この交際も田中にとっては“吉”とした。\\n\\n・週刊アサヒ芸能 ［ライト版］＜デジタル＞（PC版）\\n・週刊アサヒ芸能（モバイル版）\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://news.livedoor.com/article/detail/5039076/\\n2010-09-29T01:00:00+0900\\n予約締切は本日まで！「キン肉マン」10周年Ｔシャツイベント\\n人気漫画「キン肉マン」のアパレルシリーズが今年で10年目となる記念イヤーを迎えた。\\n\\n本シリーズをプロデュースするBAMBAMBIGELOW＆livedoorのコラボーレーション企画として、先月末より、キン肉マンに登場する超人10名のオリジナルＴシャツを期間限定で発売しており、いよいよ今日29日に、その予約注文締切日を迎える。\\n\\n10月29日（金）、日本記念日協会が定める「キン肉マンの日」（29日の金曜日）に注文したＴシャツが届く同イベント——、購入者の中から抽選で10名にプレゼントが当たる特典も付いている同企画だが、そのプレゼントは、著者のサイン入りソフビや、バッファローマンの版画といったファン必見の内容になっている。\\n\\n泣いても笑ってもラスト一日。特設サイトを是非チェックしてほしい。\\n\\n・「キン肉マン」“マッスルアパレル”コラボ10周年記念 特設サイト\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rzyz2fGersfl"
   },
   "source": [
    "## documetsの文章が膨大な為、以下の構文解析は個人的に面白い文章と感じたdocuments[2]のみについて行う\n",
    "## 出現単語をカウントして分析する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http', 1),\n",
       " ('://', 1),\n",
       " ('news', 1),\n",
       " ('.', 3),\n",
       " ('livedoor', 1),\n",
       " ('com', 1),\n",
       " ('/', 4),\n",
       " ('article', 1),\n",
       " ('detail', 1),\n",
       " ('5172424', 1),\n",
       " ('2010', 1),\n",
       " ('-', 2),\n",
       " ('11', 1),\n",
       " ('30', 2),\n",
       " ('T', 1),\n",
       " ('08', 1),\n",
       " (':', 2),\n",
       " ('00', 2),\n",
       " ('+', 1),\n",
       " ('0900', 1),\n",
       " ('Sports', 1),\n",
       " ('Watch', 1),\n",
       " ('田中', 3),\n",
       " ('里', 9),\n",
       " ('田', 9),\n",
       " ('交際', 6),\n",
       " ('アプローチ', 2),\n",
       " ('グラビア', 1),\n",
       " ('アイドル', 1),\n",
       " ('あき', 1),\n",
       " ('騎手', 1),\n",
       " ('三浦', 1),\n",
       " ('皇', 1),\n",
       " ('成', 1),\n",
       " ('お互い', 1),\n",
       " ('ブログ', 1),\n",
       " ('日', 2),\n",
       " ('東北', 1),\n",
       " ('楽天', 1),\n",
       " ('ゴールデンイーグルス', 1),\n",
       " ('エース', 2),\n",
       " ('将', 1),\n",
       " ('大', 1),\n",
       " ('タレント', 1),\n",
       " ('公', 1),\n",
       " ('もの', 2),\n",
       " ('本日', 1),\n",
       " ('火', 1),\n",
       " ('発売', 1),\n",
       " ('週刊', 3),\n",
       " ('アサヒ', 3),\n",
       " ('芸能', 5),\n",
       " ('12', 1),\n",
       " ('9', 1),\n",
       " ('号', 1),\n",
       " ('マー', 1),\n",
       " ('君', 1),\n",
       " ('ナンパ', 1),\n",
       " ('見出し', 1),\n",
       " ('両者', 1),\n",
       " ('関係', 2),\n",
       " ('者', 2),\n",
       " ('証言', 1),\n",
       " ('紹介', 1),\n",
       " ('同誌', 2),\n",
       " ('コメント', 2),\n",
       " ('デスク', 2),\n",
       " ('周囲', 2),\n",
       " ('事情', 1),\n",
       " ('当人', 1),\n",
       " ('たち', 1),\n",
       " ('の', 1),\n",
       " ('確か', 1),\n",
       " ('結婚', 1),\n",
       " ('完全', 1),\n",
       " ('視野', 1),\n",
       " ('よう', 2),\n",
       " ('新居', 1),\n",
       " ('話', 1),\n",
       " ('二', 1),\n",
       " ('人', 2),\n",
       " ('こと', 1),\n",
       " ('出', 1),\n",
       " ('2', 1),\n",
       " ('昨年', 1),\n",
       " ('末', 1),\n",
       " ('番組', 1),\n",
       " ('共演', 1),\n",
       " ('時', 1),\n",
       " ('アタック', 1),\n",
       " ('一緒', 1),\n",
       " ('食事', 1),\n",
       " ('春', 1),\n",
       " ('頃', 1),\n",
       " ('仲', 1),\n",
       " ('スザンヌ', 1),\n",
       " ('24', 1),\n",
       " ('木下', 1),\n",
       " ('優樹', 1),\n",
       " ('菜', 1),\n",
       " ('22', 1),\n",
       " ('相談', 1),\n",
       " ('次世代', 1),\n",
       " ('ゲット', 1),\n",
       " ('わけ', 1),\n",
       " ('賞味', 1),\n",
       " ('期限切れ', 1),\n",
       " ('間近', 1),\n",
       " ('球団', 1),\n",
       " ('高校', 3),\n",
       " ('時代', 2),\n",
       " ('女子', 1),\n",
       " ('生徒', 1),\n",
       " ('在学', 1),\n",
       " ('中', 1),\n",
       " ('3', 1),\n",
       " ('年間', 1),\n",
       " ('基本', 1),\n",
       " ('オクテ', 1),\n",
       " ('プロ', 1),\n",
       " ('入り', 1),\n",
       " ('後', 1),\n",
       " ('キャバクラ', 1),\n",
       " ('モテ', 1),\n",
       " ('タイプ', 2),\n",
       " ('女', 2),\n",
       " ('あしらい', 1),\n",
       " ('不慣れ', 1),\n",
       " ('ワガママ', 1),\n",
       " ('カノ', 1),\n",
       " ('苦労', 1),\n",
       " ('同年代', 1),\n",
       " ('姉さん', 1),\n",
       " ('ほう', 1),\n",
       " ('勝負', 1),\n",
       " ('世界', 1),\n",
       " ('ケース', 1),\n",
       " ('吉', 1),\n",
       " ('ライト', 1),\n",
       " ('版', 3),\n",
       " ('デジタル', 1),\n",
       " ('PC', 1),\n",
       " ('モバイル', 1)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_filters = [POSKeepFilter('名詞, 一般'), TokenCountFilter()]\n",
    "a = Analyzer(token_filters=token_filters)\n",
    "top_words = list(a.analyze(documents[2]))\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8aCgC7-rsf1"
   },
   "source": [
    "## テキストをクリーニングする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNmxlKZkrsf6"
   },
   "outputs": [],
   "source": [
    "reg1 = '<.*?>'\n",
    "reg2 = '【.*?】'\n",
    "reg3 = '[\\n\\*`\\s■◆○★●]'\n",
    "reg4 = '(http.*?\\n)'\n",
    "reg5 = '(\\d{4}-.*?\\+\\d{4})'\n",
    "\n",
    "reg_str = f'{kakko1}|{kakko2}|{tokusyu}|{url}|{date}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "田中＆里田の交際、アプローチは里田からグラビアアイドル・ほしのあき＆騎手・三浦皇成がお互いのブログで交際を認めた日、東北楽天ゴールデンイーグルスのエース・田中将大とタレント・里田まいの交際もまた公のものとなったが、本日30日（火）発売の「週刊アサヒ芸能」（12.9号）では、「マー君を“ナンパ”した里田」との見出しで、両者の交際にまつわる関係者の証言を紹介した。同誌にコメントを寄せた芸能デスクによると、「周囲の事情はさておき、当人たちが盛り上がっているのは確か。結婚も完全に視野に入れているようで、すでに新居を探しているとの話まである」という。また、二人の交際は里田からのアプローチによるものとのことで、前出の芸能デスクは、「2人は昨年末の番組共演時に、里田から猛アタック。『一緒に食事でも』と誘い、春頃にはつきあい始めた。里田は仲のいいスザンヌ（24）、木下優樹菜（22）には、早くから相談していたし、周囲にも浮かれてしゃべりまくっていた。次世代エースをゲットしたわけですから、賞味期限切れ間近の里田にすれば、してやったりでしょう」とも語っている。ちなみに、同じく同誌にコメントする球団関係者は、「高校時代は同じ高校の女子生徒と在学中の3年間、ずっとつきあっていましたね。基本はオクテ。プロ入り後にキャバクラなども覚えましたが、まぁ、モテるタイプではないし、女あしらいも不慣れ。高校時代はかなりワガママで、元カノは苦労が絶えなかったと聞いています。同年代の女より、里田のような姉さんタイプのほうが勝負の世界では向いているケースも多いですからね」と明かしており、この交際も田中にとっては“吉”とした。・週刊アサヒ芸能［ライト版］＜デジタル＞（PC版）・週刊アサヒ芸能（モバイル版）\n"
     ]
    }
   ],
   "source": [
    "document = []\n",
    "\n",
    "for i in documents:\n",
    "    remove = re.compile(reg_str)\n",
    "    document.append(re.sub(remove, '', i))\n",
    "\n",
    "print(document[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW + TFIDFでベクトル化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文章から名詞を抽出する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_only_noun(text):\n",
    "    tagger = MeCab.Tagger()\n",
    "    words = []\n",
    "    for c in tagger.parse(text).splitlines()[:-1]:\n",
    "        surface, feature = c.split('\\t')\n",
    "        pos = feature.split(',')[0]\n",
    "        if pos == '名詞':\n",
    "            words.append(surface)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 名詞を抽出しリスト化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http :// news . livedoor . com / article / detail / 5172424 / 2010 - 11 - 30 T 08 : 00 : 00 + 0900 Sports Watch 田中 里 田 交際 アプローチ 里 田 グラビア アイドル あき 騎手 三浦 皇 成 お互い ブログ 交際 日 東北 楽天 ゴールデンイーグルス エース 田中 将 大 タレント 里 田 交際 公 もの 本日 30 日 火 発売 週刊 アサヒ 芸能 12 . 9 号 マー 君 ナンパ 里 田 見出し 両者 交際 関係 者 証言 紹介 同誌 コメント 芸能 デスク 周囲 事情 当人 たち の 確か 結婚 完全 視野 よう 新居 話 二 人 交際 里 田 アプローチ もの こと 出 芸能 デスク 2 人 昨年 末 番組 共演 時 里 田 アタック 一緒 食事 春 頃 里 田 仲 スザンヌ 24 木下 優樹 菜 22 相談 周囲 次世代 エース ゲット わけ 賞味 期限切れ 間近 里 田 同誌 コメント 球団 関係 者 高校 時代 高校 女子 生徒 在学 中 3 年間 基本 オクテ プロ 入り 後 キャバクラ モテ タイプ 女 あしらい 不慣れ 高校 時代 ワガママ カノ 苦労 同年代 女 里 田 よう 姉さん タイプ ほう 勝負 世界 ケース 交際 田中 吉 週刊 アサヒ 芸能 ライト 版 デジタル PC 版 週刊 アサヒ 芸能 モバイル 版']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_list = [split_text_only_noun(documents[2])]\n",
    "message_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy配列に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http :// news . livedoor . com / article / detail / 5172424 / 2010 - 11 - 30 T 08 : 00 : 00 + 0900 Sports Watch 田中 里 田 交際 アプローチ 里 田 グラビア アイドル あき 騎手 三浦 皇 成 お互い ブログ 交際 日 東北 楽天 ゴールデンイーグルス エース 田中 将 大 タレント 里 田 交際 公 もの 本日 30 日 火 発売 週刊 アサヒ 芸能 12 . 9 号 マー 君 ナンパ 里 田 見出し 両者 交際 関係 者 証言 紹介 同誌 コメント 芸能 デスク 周囲 事情 当人 たち の 確か 結婚 完全 視野 よう 新居 話 二 人 交際 里 田 アプローチ もの こと 出 芸能 デスク 2 人 昨年 末 番組 共演 時 里 田 アタック 一緒 食事 春 頃 里 田 仲 スザンヌ 24 木下 優樹 菜 22 相談 周囲 次世代 エース ゲット わけ 賞味 期限切れ 間近 里 田 同誌 コメント 球団 関係 者 高校 時代 高校 女子 生徒 在学 中 3 年間 基本 オクテ プロ 入り 後 キャバクラ モテ タイプ 女 あしらい 不慣れ 高校 時代 ワガママ カノ 苦労 同年代 女 里 田 よう 姉さん タイプ ほう 勝負 世界 ケース 交際 田中 吉 週刊 アサヒ 芸能 ライト 版 デジタル PC 版 週刊 アサヒ 芸能 モバイル 版'],\n",
       "      dtype='<U638')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = np.array(message_list)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoWの実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "bags = count.fit_transform(docs)\n",
    "features = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag :    (0, 51)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 44)\t1\n",
      "  (0, 52)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 57)\t1\n",
      "  (0, 64)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 71)\t1\n",
      "  (0, 65)\t1\n",
      "  (0, 94)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 53)\t1\n",
      "  (0, 56)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 42)\t2\n",
      "  (0, 50)\t1\n",
      "  (0, 35)\t1\n",
      "  (0, 62)\t1\n",
      "  (0, 48)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 69)\t1\n",
      "  (0, 73)\t1\n",
      "  (0, 68)\t1\n",
      "  (0, 85)\t1\n",
      "  :\t:\n",
      "  (0, 47)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 55)\t1\n",
      "  (0, 103)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 28)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 31)\t2\n",
      "  (0, 60)\t6\n",
      "  (0, 86)\t3\n",
      "  (0, 18)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 0)\t2\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t2\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 13)\t1\n",
      "[[2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 3 1 2 2 1 1 1\n",
      "  1 1 1 2 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 6 1 1 1 1 1 2 2 1 1 1 1\n",
      "  1 1 1 1 1 2 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 5 1 1 1 1 1 3 1 2 1 1 3]]\n",
      "count.vocabulary_ :  {'http': 13, 'news': 15, 'livedoor': 14, 'com': 11, 'article': 10, 'detail': 12, '5172424': 9, '2010': 5, '11': 3, '30': 8, '08': 1, '00': 0, '0900': 2, 'sports': 17, 'watch': 18, '田中': 86, '交際': 60, 'アプローチ': 31, 'グラビア': 36, 'アイドル': 28, 'あき': 19, '騎手': 103, '三浦': 55, 'お互い': 21, 'ブログ': 47, '東北': 81, '楽天': 82, 'ゴールデンイーグルス': 40, 'エース': 32, 'タレント': 43, 'もの': 25, '本日': 80, '発売': 88, '週刊': 99, 'アサヒ': 29, '芸能': 93, '12': 4, 'マー': 49, 'ナンパ': 46, '見出し': 95, '両者': 58, '関係': 101, '証言': 97, '紹介': 91, '同誌': 66, 'コメント': 39, 'デスク': 45, '周囲': 67, '事情': 59, '当人': 74, 'たち': 23, '確か': 90, '結婚': 92, '完全': 72, '視野': 96, 'よう': 26, '新居': 75, 'こと': 22, '昨年': 76, '番組': 87, '共演': 63, 'アタック': 30, '一緒': 54, '食事': 102, 'スザンヌ': 41, '24': 7, '木下': 79, '優樹': 61, '22': 6, '相談': 89, '次世代': 83, 'ゲット': 38, 'わけ': 27, '賞味': 98, '期限切れ': 78, '間近': 100, '球団': 84, '高校': 104, '時代': 77, '女子': 70, '生徒': 85, '在学': 68, '年間': 73, '基本': 69, 'オクテ': 33, 'プロ': 48, '入り': 62, 'キャバクラ': 35, 'モテ': 50, 'タイプ': 42, 'あしらい': 20, '不慣れ': 56, 'ワガママ': 53, 'カノ': 34, '苦労': 94, '同年代': 65, '姉さん': 71, 'ほう': 24, '勝負': 64, '世界': 57, 'ケース': 37, 'ライト': 52, 'デジタル': 44, 'pc': 16, 'モバイル': 51}\n",
      "features: ['00', '08', '0900', '11', '12', '2010', '22', '24', '30', '5172424', 'article', 'com', 'detail', 'http', 'livedoor', 'news', 'pc', 'sports', 'watch', 'あき', 'あしらい', 'お互い', 'こと', 'たち', 'ほう', 'もの', 'よう', 'わけ', 'アイドル', 'アサヒ', 'アタック', 'アプローチ', 'エース', 'オクテ', 'カノ', 'キャバクラ', 'グラビア', 'ケース', 'ゲット', 'コメント', 'ゴールデンイーグルス', 'スザンヌ', 'タイプ', 'タレント', 'デジタル', 'デスク', 'ナンパ', 'ブログ', 'プロ', 'マー', 'モテ', 'モバイル', 'ライト', 'ワガママ', '一緒', '三浦', '不慣れ', '世界', '両者', '事情', '交際', '優樹', '入り', '共演', '勝負', '同年代', '同誌', '周囲', '在学', '基本', '女子', '姉さん', '完全', '年間', '当人', '新居', '昨年', '時代', '期限切れ', '木下', '本日', '東北', '楽天', '次世代', '球団', '生徒', '田中', '番組', '発売', '相談', '確か', '紹介', '結婚', '芸能', '苦労', '見出し', '視野', '証言', '賞味', '週刊', '間近', '関係', '食事', '騎手', '高校']\n",
      "['00', '08', '0900', '11', '12', '2010', '22', '24', '30', '5172424', 'article', 'com', 'detail', 'http', 'livedoor', 'news', 'pc', 'sports', 'watch', 'あき', 'あしらい', 'お互い', 'こと', 'たち', 'ほう', 'もの', 'よう', 'わけ', 'アイドル', 'アサヒ', 'アタック', 'アプローチ', 'エース', 'オクテ', 'カノ', 'キャバクラ', 'グラビア', 'ケース', 'ゲット', 'コメント', 'ゴールデンイーグルス', 'スザンヌ', 'タイプ', 'タレント', 'デジタル', 'デスク', 'ナンパ', 'ブログ', 'プロ', 'マー', 'モテ', 'モバイル', 'ライト', 'ワガママ', '一緒', '三浦', '不慣れ', '世界', '両者', '事情', '交際', '優樹', '入り', '共演', '勝負', '同年代', '同誌', '周囲', '在学', '基本', '女子', '姉さん', '完全', '年間', '当人', '新居', '昨年', '時代', '期限切れ', '木下', '本日', '東北', '楽天', '次世代', '球団', '生徒', '田中', '番組', '発売', '相談', '確か', '紹介', '結婚', '芸能', '苦労', '見出し', '視野', '証言', '賞味', '週刊', '間近', '関係', '食事', '騎手', '高校']\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('bag : ', bags)\n",
    "print(bags.toarray())\n",
    "print(\"count.vocabulary_ : \", count.vocabulary_)\n",
    "print('features:', features)\n",
    "print(features)\n",
    "print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoWの結果からTFIDFを計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 104)\t0.19569842191603265\n",
      "  (0, 103)\t0.06523280730534423\n",
      "  (0, 102)\t0.06523280730534423\n",
      "  (0, 101)\t0.13046561461068845\n",
      "  (0, 100)\t0.06523280730534423\n",
      "  (0, 99)\t0.19569842191603265\n",
      "  (0, 98)\t0.06523280730534423\n",
      "  (0, 97)\t0.06523280730534423\n",
      "  (0, 96)\t0.06523280730534423\n",
      "  (0, 95)\t0.06523280730534423\n",
      "  (0, 94)\t0.06523280730534423\n",
      "  (0, 93)\t0.3261640365267211\n",
      "  (0, 92)\t0.06523280730534423\n",
      "  (0, 91)\t0.06523280730534423\n",
      "  (0, 90)\t0.06523280730534423\n",
      "  (0, 89)\t0.06523280730534423\n",
      "  (0, 88)\t0.06523280730534423\n",
      "  (0, 87)\t0.06523280730534423\n",
      "  (0, 86)\t0.19569842191603265\n",
      "  (0, 85)\t0.06523280730534423\n",
      "  (0, 84)\t0.06523280730534423\n",
      "  (0, 83)\t0.06523280730534423\n",
      "  (0, 82)\t0.06523280730534423\n",
      "  (0, 81)\t0.06523280730534423\n",
      "  (0, 80)\t0.06523280730534423\n",
      "  :\t:\n",
      "  (0, 24)\t0.06523280730534423\n",
      "  (0, 23)\t0.06523280730534423\n",
      "  (0, 22)\t0.06523280730534423\n",
      "  (0, 21)\t0.06523280730534423\n",
      "  (0, 20)\t0.06523280730534423\n",
      "  (0, 19)\t0.06523280730534423\n",
      "  (0, 18)\t0.06523280730534423\n",
      "  (0, 17)\t0.06523280730534423\n",
      "  (0, 16)\t0.06523280730534423\n",
      "  (0, 15)\t0.06523280730534423\n",
      "  (0, 14)\t0.06523280730534423\n",
      "  (0, 13)\t0.06523280730534423\n",
      "  (0, 12)\t0.06523280730534423\n",
      "  (0, 11)\t0.06523280730534423\n",
      "  (0, 10)\t0.06523280730534423\n",
      "  (0, 9)\t0.06523280730534423\n",
      "  (0, 8)\t0.13046561461068845\n",
      "  (0, 7)\t0.06523280730534423\n",
      "  (0, 6)\t0.06523280730534423\n",
      "  (0, 5)\t0.06523280730534423\n",
      "  (0, 4)\t0.06523280730534423\n",
      "  (0, 3)\t0.06523280730534423\n",
      "  (0, 2)\t0.06523280730534423\n",
      "  (0, 1)\t0.06523280730534423\n",
      "  (0, 0)\t0.13046561461068845\n",
      "[[0.13 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.13 0.07 0.07 0.07 0.07 0.07\n",
      "  0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.13 0.13 0.07\n",
      "  0.07 0.2  0.07 0.13 0.13 0.07 0.07 0.07 0.07 0.07 0.07 0.13 0.07 0.07\n",
      "  0.13 0.07 0.07 0.13 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07\n",
      "  0.07 0.07 0.07 0.07 0.39 0.07 0.07 0.07 0.07 0.07 0.13 0.13 0.07 0.07\n",
      "  0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.13 0.07 0.07 0.07 0.07 0.07 0.07\n",
      "  0.07 0.07 0.2  0.07 0.07 0.07 0.07 0.07 0.07 0.33 0.07 0.07 0.07 0.07\n",
      "  0.07 0.2  0.07 0.13 0.07 0.07 0.2 ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "tf_idf = tfidf.fit_transform(bags)\n",
    "print(tf_idf)\n",
    "print(tf_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## あるニュースに一番cos類似度が近いニュースを出力する関数の作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## documentsの中から指定した文章に一番cos類似度が高い文章を出力する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_news_with_cos_similarity(source_id):\n",
    "    \n",
    "    # 文字列を単語で分割しリストに格納する\n",
    "    def word_tokenaize(doc):\n",
    "        tagger = MeCab.Tagger('-Owakati')\n",
    "        # 初期化しないとエラーになる\n",
    "        tagger.parse(\"\")\n",
    "        \n",
    "        node = tagger.parseToNode(doc)\n",
    "\n",
    "        result = []\n",
    "        while node:\n",
    "            hinshi = node.feature.split(\",\")[0]\n",
    "            if hinshi == '名詞' or hinshi == '動詞':\n",
    "                result.append(node.feature.split(\",\")[6])\n",
    "            node = node.next\n",
    "\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def words_to_freqdict(words):\n",
    "        \"\"\"\n",
    "        単語の配列を、単語と頻度の辞書に変換する関数\n",
    "        例: [\"X\",\"X\",\"Y\",\"Z\",\"X\"] => {\"X\":3, \"Y\":1, \"Z\":1}\n",
    "        @param words 単語の配列\n",
    "        @return 単語と頻度の辞書\n",
    "        \"\"\"\n",
    "        freqdict = {}\n",
    "        for word in words:\n",
    "            if word in freqdict:\n",
    "                freqdict[word] = freqdict[word] + 1\n",
    "            else:\n",
    "                freqdict[word] = 1\n",
    "        return freqdict\n",
    "    \n",
    "    \n",
    "    def calc_cos(dictA, dictB):\n",
    "        \"\"\"\n",
    "        cos類似度を計算する関数\n",
    "        @param dictA 1つ目の文章\n",
    "        @param dictB 2つ目の文章\n",
    "        @return cos類似度を計算した結果。0〜1で1に近ければ類似度が高い。\n",
    "        \"\"\"\n",
    "        # 文書Aのベクトル長を計算\n",
    "        lengthA = 0.0\n",
    "        for key,value in dictA.items():\n",
    "            lengthA = lengthA + value*value\n",
    "        lengthA = math.sqrt(lengthA)\n",
    "\n",
    "        # 文書Bのベクトル長を計算\n",
    "        lengthB = 0.0\n",
    "        for key,value in dictB.items():\n",
    "            lengthB = lengthB + value*value\n",
    "        lengthB = math.sqrt(lengthB)\n",
    "\n",
    "        # AとBの内積を計算\n",
    "        dotProduct = 0.0\n",
    "        for keyA,valueA in dictA.items():\n",
    "            for keyB,valueB in dictB.items():\n",
    "                if keyA==keyB:\n",
    "                    dotProduct = dotProduct + valueA*valueB\n",
    "        # cos類似度を計算\n",
    "        cos = dotProduct / (lengthA*lengthB)\n",
    "        return cos\n",
    "    \n",
    "    \n",
    "    def compute_cos_sim(s_id, d_id):\n",
    "        source_list = word_tokenaize(documents[s_id])\n",
    "        destination_list = word_tokenaize(documents[d_id])\n",
    "        \n",
    "        source_freq_list = words_to_freqdict(source_list)\n",
    "        destination_freq_list = words_to_freqdict(destination_list)\n",
    "        \n",
    "        cos_sim = calc_cos(source_freq_list, destination_freq_list)\n",
    "        \n",
    "        return cos_sim\n",
    "    \n",
    "    def main():\n",
    "        documents = bin_data.data\n",
    "        document_num = len(documents)\n",
    "\n",
    "        print('比較元の文章 documents[{}]'.format(source_id))\n",
    "        print('{}'.format(documents[int(source_id)]))\n",
    "\n",
    "        similer_document_id = 0\n",
    "        cos_sim = compute_cos_sim(int(source_id), 0)\n",
    "\n",
    "        for index in range(document_num):\n",
    "            test_cos_sim = compute_cos_sim(int(source_id), int(index))\n",
    "\n",
    "            if source_id != index:\n",
    "                if cos_sim < test_cos_sim:\n",
    "                    cos_sim = test_cos_sim\n",
    "                    similer_document_id = index\n",
    "\n",
    "        print('最もcos類似度が高い文章　documents[{}]'.format(similer_document_id))\n",
    "        print('cos類似度={}'.format(cos_sim))\n",
    "        print('{}'.format(documents[int(similer_document_id)]))\n",
    "\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動作検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "比較元の文章 documents[2]\n",
      "http://news.livedoor.com/article/detail/5172424/\n",
      "2010-11-30T08:00:00+0900\n",
      "【Sports Watch】田中＆里田の交際、アプローチは里田から\n",
      "グラビアアイドル・ほしのあき＆騎手・三浦皇成がお互いのブログで交際を認めた日、東北楽天ゴールデンイーグルスのエース・田中将大とタレント・里田まいの交際もまた公のものとなったが、本日30日（火）発売の「週刊アサヒ芸能」（12.9号）では、「マー君を“ナンパ”した里田」との見出しで、両者の交際にまつわる関係者の証言を紹介した。\n",
      "\n",
      "同誌にコメントを寄せた芸能デスクによると、「周囲の事情はさておき、当人たちが盛り上がっているのは確か。結婚も完全に視野に入れているようで、すでに新居を探しているとの話まである」という。\n",
      "\n",
      "また、二人の交際は里田からのアプローチによるものとのことで、前出の芸能デスクは、「2人は昨年末の番組共演時に、里田から猛アタック。『一緒に食事でも』と誘い、春頃にはつきあい始めた。里田は仲のいいスザンヌ（24）、木下優樹菜（22）には、早くから相談していたし、周囲にも浮かれてしゃべりまくっていた。次世代エースをゲットしたわけですから、賞味期限切れ間近の里田にすれば、してやったりでしょう」とも語っている。\n",
      "\n",
      "ちなみに、同じく同誌にコメントする球団関係者は、「高校時代は同じ高校の女子生徒と在学中の3年間、ずっとつきあっていましたね。基本はオクテ。プロ入り後にキャバクラなども覚えましたが、まぁ、モテるタイプではないし、女あしらいも不慣れ。高校時代はかなりワガママで、元カノは苦労が絶えなかったと聞いています。同年代の女より、里田のような姉さんタイプのほうが勝負の世界では向いているケースも多いですからね」と明かしており、この交際も田中にとっては“吉”とした。\n",
      "\n",
      "・週刊アサヒ芸能 ［ライト版］＜デジタル＞（PC版）\n",
      "・週刊アサヒ芸能（モバイル版）\n",
      "\n",
      "最もcos類似度が高い文章　documents[7190]\n",
      "cos類似度=0.9398907269721967\n",
      "http://news.livedoor.com/article/detail/5377551/\n",
      "2011-03-01T08:00:00+0900\n",
      "【Sports Watch】里田まい、マー君との交際には“おばかキャラは一切なし”\n",
      "本日1日（火）発売の「週刊アサヒ芸能」（3.10）では、『美女リークス』なる見出しで、小林麻央や綾瀬はるか、平井理央といった旬な女優やアナウンサー、美女アスリートの近況を伝えている。スポーツ系でいえば、昨年11月末、東北楽天ゴールデンイーグルスのエース・田中将大とタレント・里田まいの交際が公のものとなったが、その後も順調に続いているという二人の様子を里田側からフューチャーした。\n",
      "\n",
      "同誌にコメントを寄せるスポーツ紙のデスクは、「里田は、マー君の話になると、ふだんのおばかキャラは一切なし。しっかりとした“姉さん女房”なんです。その姿に楽天関係者からの評価も上がりっぱなしですね」と語っており、マー君をリードしているのだとか。\n",
      "\n",
      "また、結婚は時間の問題と言われている二人について、楽天の番記者は「2月のキャンプ中は会えないので電話やメールで連絡を取り合っていた。特にメールは練習が終わるや、速攻で返信しているそうです。そのメール打ちの速度たるや、女子高校生のように巧みで、物凄い速さだとか（笑）。里田にそこまで成長させられたか、と先輩や同僚選手にからかわれると、生真面目な顔を真っ赤にして『スイマセン』と謝っていたそうです」と語り、前出のデスクは「これまでリップサービスをしてくれなかった田中が、エース岩隈の前で、あの『開幕投手宣言』でしたからね。結果を出して、オフに結婚なんて約束をしているのかもしれません」と続けている。\n",
      "\n",
      "並々ならぬ意気込みで臨むマー君の今シーズン。斎藤佑樹ら黄金世代とのライバル対決も、愛の力で乗り越えてしまうのか——。\n",
      "\n",
      "・週刊アサヒ芸能 ［ライト版］＜デジタル＞（PC版）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_news_with_cos_similarity(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-2tdtjcrsgD"
   },
   "source": [
    "## 別の類似度手法を1つ調べて上の関数に組み込む(切り替えられるようにする)\n",
    "\n",
    "### ◯Word Mover's Distanceを導入\n",
    "#### Word Mover's Distanceとは\n",
    "- Word2vec や GloVe 等で得られた単語の分散表現を使って文書間の距離を計算する手法\n",
    "- 値が0に近いほど似た文章であるということを示している　※cos類似度は1に近いほど似た文章となる\n",
    "- Twitterのようなショートテキストに対して有効という研究結果が多い為、今回のような長い文章では対応できない可能性あり\n",
    "\n",
    "#### 導入にあたっての準備\n",
    "- 使用する分散表現は以下のページから学習済みのデータ(GoogleNews-vectors-negative300.bin.gz)をダウンロードし使用する\n",
    "- 上記のデータは単語ベクトルは300次元で、ボキャブラリ数が300万、学習にはGoogle Newsコーパス(1000億語)を使用している。\n",
    "\n",
    "[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動作例「japanese」にWMDが近い単語を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('japan', 0.6607722640037537),\n",
       " ('chinese', 0.6502295732498169),\n",
       " ('Japanese', 0.6149078607559204),\n",
       " ('korean', 0.6051568984985352),\n",
       " ('german', 0.5999273061752319),\n",
       " ('american', 0.5906797647476196),\n",
       " ('asian', 0.5839767456054688),\n",
       " ('san', 0.5834757089614868),\n",
       " ('jap', 0.5764404535293579),\n",
       " ('swedish', 0.5720360279083252)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['japanese'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数にWord Mover's Distanceを組み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOyUrKfursgE"
   },
   "outputs": [],
   "source": [
    "def output_news_with_cos_similarity(source_id, method):\n",
    "    \n",
    "    # 文字列を単語で分割しリストに格納する\n",
    "    def word_tokenaize(doc):\n",
    "        tagger = MeCab.Tagger('-Owakati')\n",
    "        # 初期化しないとエラーになる\n",
    "        tagger.parse(\"\")\n",
    "        \n",
    "        node = tagger.parseToNode(doc)\n",
    "\n",
    "        result = []\n",
    "        while node:\n",
    "            hinshi = node.feature.split(\",\")[0]\n",
    "            if hinshi == '名詞' or hinshi == '動詞':\n",
    "                result.append(node.feature.split(\",\")[6])\n",
    "            node = node.next\n",
    "\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def words_to_freqdict(words):\n",
    "        \"\"\"\n",
    "        単語の配列を、単語と頻度の辞書に変換する関数\n",
    "        例: [\"X\",\"X\",\"Y\",\"Z\",\"X\"] => {\"X\":3, \"Y\":1, \"Z\":1}\n",
    "        @param words 単語の配列\n",
    "        @return 単語と頻度の辞書\n",
    "        \"\"\"\n",
    "        freqdict = {}\n",
    "        for word in words:\n",
    "            if word in freqdict:\n",
    "                freqdict[word] = freqdict[word] + 1\n",
    "            else:\n",
    "                freqdict[word] = 1\n",
    "        return freqdict\n",
    "    \n",
    "    \n",
    "    def calc_cos(dictA, dictB):\n",
    "        \"\"\"\n",
    "        cos類似度を計算する関数\n",
    "        @param dictA 1つ目の文章\n",
    "        @param dictB 2つ目の文章\n",
    "        @return cos類似度を計算した結果。0〜1で1に近ければ類似度が高い。\n",
    "        \"\"\"\n",
    "        # 文書Aのベクトル長を計算\n",
    "        lengthA = 0.0\n",
    "        for key,value in dictA.items():\n",
    "            lengthA = lengthA + value*value\n",
    "        lengthA = math.sqrt(lengthA)\n",
    "\n",
    "        # 文書Bのベクトル長を計算\n",
    "        lengthB = 0.0\n",
    "        for key,value in dictB.items():\n",
    "            lengthB = lengthB + value*value\n",
    "        lengthB = math.sqrt(lengthB)\n",
    "\n",
    "        # AとBの内積を計算\n",
    "        dotProduct = 0.0\n",
    "        for keyA,valueA in dictA.items():\n",
    "            for keyB,valueB in dictB.items():\n",
    "                if keyA==keyB:\n",
    "                    dotProduct = dotProduct + valueA*valueB\n",
    "        # cos類似度を計算\n",
    "        cos = dotProduct / (lengthA*lengthB)\n",
    "        return cos\n",
    "    \n",
    "    \n",
    "    def compute_cos_sim(s_id, d_id):\n",
    "        source_list = word_tokenaize(documents[s_id])\n",
    "        destination_list = word_tokenaize(documents[d_id])\n",
    "        \n",
    "        source_freq_list = words_to_freqdict(source_list)\n",
    "        destination_freq_list = words_to_freqdict(destination_list)\n",
    "        \n",
    "        cos_sim = calc_cos(source_freq_list, destination_freq_list)\n",
    "        \n",
    "        return cos_sim\n",
    "    \n",
    "\n",
    "    \n",
    "    def main():\n",
    "        documents = bin_data.data\n",
    "        document_num = len(documents)\n",
    "        print('使用する類似度手法 {}'.format(method))\n",
    "        print('比較元の文章 documents[{}]'.format(source_id))\n",
    "        print('{}'.format(documents[int(source_id)]))\n",
    "\n",
    "        similer_document_id = 0\n",
    "        \n",
    "        if method == 'cos':\n",
    "            sim = compute_cos_sim(int(source_id), 0)\n",
    "\n",
    "            for index in range(document_num):\n",
    "                test_cos_sim = compute_cos_sim(int(source_id), int(index))\n",
    "\n",
    "                if source_id != index:\n",
    "                    if sim < test_cos_sim:\n",
    "                        sim = test_cos_sim\n",
    "                        similer_document_id = index\n",
    "        \n",
    "        if method == 'wmd':\n",
    "            sim = model.wmdistance(documents[int(source_id)], documents[0])\n",
    "            \n",
    "            for index in range(document_num):\n",
    "                test_wmd_sim = model.wmdistance(documents[int(source_id)], documents[int(index)])\n",
    "\n",
    "                if source_id != index:\n",
    "                    if sim > test_wmd_sim:\n",
    "                        sim = test_wmd_sim\n",
    "                        similer_document_id = index\n",
    "            \n",
    "\n",
    "        print('最も類似度が高い文章　documents[{}]'.format(similer_document_id))\n",
    "        print('類似度={}'.format(sim))\n",
    "        print('{}'.format(documents[int(similer_document_id)]))\n",
    "\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cos類似度を使用した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用する類似度手法 cos\n",
      "比較元の文章 documents[2]\n",
      "http://news.livedoor.com/article/detail/5172424/\n",
      "2010-11-30T08:00:00+0900\n",
      "【Sports Watch】田中＆里田の交際、アプローチは里田から\n",
      "グラビアアイドル・ほしのあき＆騎手・三浦皇成がお互いのブログで交際を認めた日、東北楽天ゴールデンイーグルスのエース・田中将大とタレント・里田まいの交際もまた公のものとなったが、本日30日（火）発売の「週刊アサヒ芸能」（12.9号）では、「マー君を“ナンパ”した里田」との見出しで、両者の交際にまつわる関係者の証言を紹介した。\n",
      "\n",
      "同誌にコメントを寄せた芸能デスクによると、「周囲の事情はさておき、当人たちが盛り上がっているのは確か。結婚も完全に視野に入れているようで、すでに新居を探しているとの話まである」という。\n",
      "\n",
      "また、二人の交際は里田からのアプローチによるものとのことで、前出の芸能デスクは、「2人は昨年末の番組共演時に、里田から猛アタック。『一緒に食事でも』と誘い、春頃にはつきあい始めた。里田は仲のいいスザンヌ（24）、木下優樹菜（22）には、早くから相談していたし、周囲にも浮かれてしゃべりまくっていた。次世代エースをゲットしたわけですから、賞味期限切れ間近の里田にすれば、してやったりでしょう」とも語っている。\n",
      "\n",
      "ちなみに、同じく同誌にコメントする球団関係者は、「高校時代は同じ高校の女子生徒と在学中の3年間、ずっとつきあっていましたね。基本はオクテ。プロ入り後にキャバクラなども覚えましたが、まぁ、モテるタイプではないし、女あしらいも不慣れ。高校時代はかなりワガママで、元カノは苦労が絶えなかったと聞いています。同年代の女より、里田のような姉さんタイプのほうが勝負の世界では向いているケースも多いですからね」と明かしており、この交際も田中にとっては“吉”とした。\n",
      "\n",
      "・週刊アサヒ芸能 ［ライト版］＜デジタル＞（PC版）\n",
      "・週刊アサヒ芸能（モバイル版）\n",
      "\n",
      "最もcos類似度が高い文章　documents[7190]\n",
      "類似度=0.9398907269721967\n",
      "http://news.livedoor.com/article/detail/5377551/\n",
      "2011-03-01T08:00:00+0900\n",
      "【Sports Watch】里田まい、マー君との交際には“おばかキャラは一切なし”\n",
      "本日1日（火）発売の「週刊アサヒ芸能」（3.10）では、『美女リークス』なる見出しで、小林麻央や綾瀬はるか、平井理央といった旬な女優やアナウンサー、美女アスリートの近況を伝えている。スポーツ系でいえば、昨年11月末、東北楽天ゴールデンイーグルスのエース・田中将大とタレント・里田まいの交際が公のものとなったが、その後も順調に続いているという二人の様子を里田側からフューチャーした。\n",
      "\n",
      "同誌にコメントを寄せるスポーツ紙のデスクは、「里田は、マー君の話になると、ふだんのおばかキャラは一切なし。しっかりとした“姉さん女房”なんです。その姿に楽天関係者からの評価も上がりっぱなしですね」と語っており、マー君をリードしているのだとか。\n",
      "\n",
      "また、結婚は時間の問題と言われている二人について、楽天の番記者は「2月のキャンプ中は会えないので電話やメールで連絡を取り合っていた。特にメールは練習が終わるや、速攻で返信しているそうです。そのメール打ちの速度たるや、女子高校生のように巧みで、物凄い速さだとか（笑）。里田にそこまで成長させられたか、と先輩や同僚選手にからかわれると、生真面目な顔を真っ赤にして『スイマセン』と謝っていたそうです」と語り、前出のデスクは「これまでリップサービスをしてくれなかった田中が、エース岩隈の前で、あの『開幕投手宣言』でしたからね。結果を出して、オフに結婚なんて約束をしているのかもしれません」と続けている。\n",
      "\n",
      "並々ならぬ意気込みで臨むマー君の今シーズン。斎藤佑樹ら黄金世代とのライバル対決も、愛の力で乗り越えてしまうのか——。\n",
      "\n",
      "・週刊アサヒ芸能 ［ライト版］＜デジタル＞（PC版）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_news_with_cos_similarity(2, 'cos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Mover's Distanceを使用した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2Q1u0TarsgF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用する類似度手法 wmd\n",
      "比較元の文章 documents[2]\n",
      "http://news.livedoor.com/article/detail/5172424/\n",
      "2010-11-30T08:00:00+0900\n",
      "【Sports Watch】田中＆里田の交際、アプローチは里田から\n",
      "グラビアアイドル・ほしのあき＆騎手・三浦皇成がお互いのブログで交際を認めた日、東北楽天ゴールデンイーグルスのエース・田中将大とタレント・里田まいの交際もまた公のものとなったが、本日30日（火）発売の「週刊アサヒ芸能」（12.9号）では、「マー君を“ナンパ”した里田」との見出しで、両者の交際にまつわる関係者の証言を紹介した。\n",
      "\n",
      "同誌にコメントを寄せた芸能デスクによると、「周囲の事情はさておき、当人たちが盛り上がっているのは確か。結婚も完全に視野に入れているようで、すでに新居を探しているとの話まである」という。\n",
      "\n",
      "また、二人の交際は里田からのアプローチによるものとのことで、前出の芸能デスクは、「2人は昨年末の番組共演時に、里田から猛アタック。『一緒に食事でも』と誘い、春頃にはつきあい始めた。里田は仲のいいスザンヌ（24）、木下優樹菜（22）には、早くから相談していたし、周囲にも浮かれてしゃべりまくっていた。次世代エースをゲットしたわけですから、賞味期限切れ間近の里田にすれば、してやったりでしょう」とも語っている。\n",
      "\n",
      "ちなみに、同じく同誌にコメントする球団関係者は、「高校時代は同じ高校の女子生徒と在学中の3年間、ずっとつきあっていましたね。基本はオクテ。プロ入り後にキャバクラなども覚えましたが、まぁ、モテるタイプではないし、女あしらいも不慣れ。高校時代はかなりワガママで、元カノは苦労が絶えなかったと聞いています。同年代の女より、里田のような姉さんタイプのほうが勝負の世界では向いているケースも多いですからね」と明かしており、この交際も田中にとっては“吉”とした。\n",
      "\n",
      "・週刊アサヒ芸能 ［ライト版］＜デジタル＞（PC版）\n",
      "・週刊アサヒ芸能（モバイル版）\n",
      "\n",
      "最もcos類似度が高い文章　documents[5666]\n",
      "類似度=0.33731426517908353\n",
      "http://news.livedoor.com/article/detail/5225245/\n",
      "2010-12-22T10:30:00+0900\n",
      "【Sports Watch】里田まい、マー君との結婚を機に独立か!?\n",
      "先月、東北楽天ゴールデンイーグルス・田中将大と人気タレント、里田まいの熱愛報道が世間を騒がせたが、その後も二人は、バリ島に極秘旅行をするなど、交際は順調の様子だ。\n",
      "\n",
      "そんな折、21日（火）発売の「週刊アサヒ芸能」（12.30/1.6号）では、『里田まい 「マー君と結婚」で独立を画策！「ポスト紗栄子を狙っています」』との見出しで、熱愛報道の裏側を伝えた。\n",
      "\n",
      "「実は、マー君のほうが完全に里田にメロメロ。シーズンの最後のほうでは、毎試合後に、都内にいる里田に『帰るメール』までしていました。マー君自身、周囲に『いつ結婚してもいい』と話しており、1人暮らしのマンションの一角はすでに『里田コーナー』ができていて、私物も搬入済みだとか。里田も仕事がない時には頻繁にお泊まりもしているようです」。同誌に掲載された楽天関係者の談話である。\n",
      "\n",
      "だが、同じく同誌にコメントを寄せる芸能関係者は、「実は結婚を機に、所属事務所から里田を独立させようとしている芸能関係者がいるんです。里田も信頼を寄せている人物で、これには、周囲もかなり困惑しているようです」と明かし、「これは、里田に近い関係者が漏らしていたようなんですが、現在、ダルビッシュと離婚協議中の紗栄子がかつて持っていたイメージの“よき人妻”キャラとして売り出そうというのです。結婚を機に、芸能活動をセーブ、そして、結婚生活をブログなどで小出しにしていく作戦のようですね」とも語っている。\n",
      "\n",
      "ヘキサゴンブームにより、人気に陰りがみえていたといわれる里田。マー君との交際は事務所にとっても吉報だったと言われているのだが、結婚を機に独立をされてはたまったものではないだろう。詳細は同誌をチェックしてほしい。\n",
      "\n",
      "・週刊アサヒ芸能 ［ライト版］＜デジタル＞（PC版）\n",
      "・週刊アサヒ芸能（モバイル版）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_news_with_cos_similarity(2, 'wmd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PW1NmCjNrsgG"
   },
   "source": [
    "## なぜそのような結果になったのか考察する\n",
    "\n",
    "\n",
    "### Cos類似度の出力結果について\n",
    "#### 比較元に記載されている単語の一部が、出力された文章にも記載されている\n",
    "- 例　マー君、里田、東北楽天ゴールデンイーグルス、週刊アサヒ芸能、関係者\n",
    "\n",
    "#### 想定される理由\n",
    "- 動作原理として文章の全単語をベクトル化したデータから、類似した文章を選択している為、同様の単語を多く含む文章が選択されたと思われる。\n",
    "\n",
    "### Word Mover's Distanceの出力結果について\n",
    "#### 文章の位置と内容が近い文章が出力されている\n",
    "- 例　比較元16行目「ちなみに、同じく同誌にコメントする球団関係者は、」出力14行目「同じく同誌に寄せる芸能関係者は、」\n",
    "\n",
    "#### 想定される理由\n",
    "- 動作原理として文書Aを文書Bへ変換する際のコストが最小となる組み合わせを選択している為、同じ位置に同じような文章を含む文章が最も変換ロスが小さい文章と推定されていると思われる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z4Yra1eSrsgJ"
   },
   "source": [
    "# 【問題6】感情分析\n",
    "目的\n",
    "\n",
    "- NLP定番の感情分析の経験\n",
    "- 英語の処理の実践\n",
    "以下からLarge Movie Review Datasetをダウンロードしてください。\n",
    "\n",
    "[IMDBレビュー](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "\n",
    "同じようにwgetコマンドでも可能です。\n",
    "\n",
    "```python\n",
    "# IMDBをカレントフォルダにダウンロード\n",
    "!wget 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "# 解凍\n",
    "!tar 'zxf aclImdb_v1.tar.gz'\n",
    "# aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf 'aclImdb/train/unsup'\n",
    "# IMDBデータセットの説明を表示\n",
    "!cat 'aclImdb/README'\n",
    "```\n",
    "\n",
    "```python\n",
    "# サブフォルダまで自動で読み込んでもらう\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "train_text, train_y = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "test_text, test_y = test_review.data, test_review.target\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TV6yy2Uursga"
   },
   "source": [
    "## 【問】\n",
    "IMDBという映画に対するレビューのデータセットを使います。\n",
    "良いレビューか悪いレビューかを判定するモデルを作ってください。\n",
    "テストデータに対する正解率が90%を超えるまで、調査=>実行=>改善を繰り返してください。\n",
    "前処理になぜその処理をしたのかを書くとエンジニアリングとしても完璧です。\n",
    "注意: 必ず間違っていたデータを観察してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文章が記載されたデータ\n",
    "- 作成したモデルで正確に分類出来なかった文章を確認する際に使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "test_text, test_y = test_review.data, test_review.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't hate Heather Graham because she's beautiful, hate her because she's fun to watch in this movie. Like the hip clothing and funky surroundings, the actors in this flick work well together. Casey Affleck is hysterical and Heather Graham literally lights up the screen. The minor characters - Goran Visnjic {sigh} and Patricia Velazquez are as TALENTED as they are gorgeous. Congratulations Miramax & Director Lisa Krueger!\""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数値化されたデータ\n",
    "- 学習に使用する\n",
    "- IMDBデータセットをダウンロードし、学習データを配列trainに、テストデータを配列testへ格納\n",
    "- 各配列[0]がレビューの単語データ、各配列[1]がレビューの感情ラベル\n",
    "- 単語データはインデックス化されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.iro.umontreal.ca/~lisa/deep/data/imdb.pkl\n"
     ]
    }
   ],
   "source": [
    "train, test, _ = imdb.load_data(path='./data/imdb.pkl', n_words=10000, valid_portion=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理\n",
    "- 単語データと感情ラベルを別々の配列に分ける（XとY）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用とテスト用のデータをそれぞれ変数に格納\n",
    "X_train, y_train = train\n",
    "X_test, y_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 単語データの配列Xはサイズを合わせる（ゼロパディング）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用とテスト用の単語データのサイズを合わせる\n",
    "X_train = pad_sequences(X_train, maxlen=100, value=0.)\n",
    "X_test = pad_sequences(X_test, maxlen=100, value=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 感情ラベルYは0（肯定）か1（否定）のどちらか2値を持つ、このデータをone-hot-encodingする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 17,  25,  10, 406,  26,  14,  56,  61,  62, 323,   4,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0]),\n",
       " array([  70,  922,   87,  394,   25,    3,  129,  883,   53,  457,   86,\n",
       "         879,   87,   70,  297,   42,   41,   86, 1752,   14,   40,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1つ目のレビュー単語データの中身を確認\n",
    "X_train[0], X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正解データのサイズを合わせる\n",
    "y_train = to_categorical(y_train, nb_classes=2)\n",
    "y_test = to_categorical(y_test, nb_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0.]), array([1., 0.]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1つ目のレビュー正解データの中身を確認\n",
    "y_train[0], y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの作成\n",
    "- 入力層、中間層（埋め込み層、LSTMブロック）、出力層を作成し、学習条件を設定\n",
    "- 最適化手法としてAdam Optimizerを使用して重みを更新\n",
    "- 埋め込み層では、単語の特徴を数値で表現したものが出力される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力層の作成\n",
    "net = tflearn.input_data([None, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中間層の作成\n",
    "# 単語埋め込み層の作成\n",
    "net = tflearn.embedding(net, input_dim=10000, output_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\takahiro\\Anaconda3\\lib\\site-packages\\tflearn\\layers\\recurrent.py:69: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\takahiro\\Anaconda3\\lib\\site-packages\\tflearn\\layers\\recurrent.py:681: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# LSTMブロック\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力層の作成\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\takahiro\\Anaconda3\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# 学習条件のセット\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの作成\n",
    "- 作成したニューラルネットワークに対しデータをセットし学習を実行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\takahiro\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# モデルのセット\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証結果　Test Accuracy 0.8228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7039  | total loss: \u001b[1m\u001b[32m0.06559\u001b[0m\u001b[0m | time: 99.775s\n",
      "| Adam | epoch: 010 | loss: 0.06559 - acc: 0.9903 -- iter: 22496/22500\n",
      "Training Step: 7040  | total loss: \u001b[1m\u001b[32m0.08002\u001b[0m\u001b[0m | time: 103.388s\n",
      "| Adam | epoch: 010 | loss: 0.08002 - acc: 0.9881 | val_loss: 0.64915 - val_acc: 0.8228 -- iter: 22500/22500\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# 学習の実行\n",
    "model.fit(X_train, y_train, validation_set=(X_test, y_test), show_metric=True, batch_size=32, run_id='lstm_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 間違っている文章を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9907098 , 0.00929019],\n",
       "       [0.9799039 , 0.02009615],\n",
       "       [0.01865811, 0.98134196],\n",
       "       ...,\n",
       "       [0.06731224, 0.9326878 ],\n",
       "       [0.6471061 , 0.3528939 ],\n",
       "       [0.00197308, 0.9980269 ]], dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array(model.predict(X_test))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_index = y_pred.argmax(axis=1)\n",
    "y_pred_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test.argmax(axis=1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 間違えた文章のindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   5,   18,   26,   40,   46,   57,   58,   63,   66,   67,   69,\n",
       "          89,   92,  136,  141,  150,  154,  159,  163,  166,  170,  178,\n",
       "         194,  205,  206,  207,  212,  225,  241,  260,  265,  269,  281,\n",
       "         282,  285,  290,  292,  300,  304,  331,  337,  338,  342,  344,\n",
       "         346,  354,  355,  358,  360,  372,  373,  381,  382,  385,  395,\n",
       "         403,  406,  410,  415,  418,  440,  451,  458,  461,  492,  494,\n",
       "         499,  500,  507,  526,  529,  554,  562,  585,  591,  608,  614,\n",
       "         615,  626,  629,  630,  633,  638,  649,  657,  664,  670,  674,\n",
       "         676,  678,  681,  683,  686,  700,  705,  708,  713,  714,  724,\n",
       "         727,  737,  741,  751,  758,  764,  769,  772,  794,  798,  800,\n",
       "         801,  804,  810,  811,  816,  817,  853,  856,  866,  884,  895,\n",
       "         897,  899,  905,  907,  909,  923,  925,  927,  939,  943,  958,\n",
       "         960,  966,  979,  980,  984,  986,  987,  995, 1003, 1004, 1007,\n",
       "        1010, 1015, 1016, 1021, 1024, 1038, 1041, 1042, 1045, 1046, 1049,\n",
       "        1052, 1054, 1055, 1064, 1084, 1085, 1086, 1094, 1098, 1100, 1102,\n",
       "        1112, 1130, 1139, 1144, 1152, 1156, 1166, 1169, 1179, 1194, 1205,\n",
       "        1213, 1217, 1226, 1228, 1240, 1241, 1245, 1252, 1254, 1262, 1264,\n",
       "        1267, 1280, 1286, 1287, 1294, 1310, 1312, 1313, 1314, 1318, 1321,\n",
       "        1333, 1335, 1339, 1341, 1345, 1349, 1354, 1356, 1359, 1360, 1361,\n",
       "        1362, 1363, 1365, 1369, 1376, 1377, 1390, 1393, 1410, 1425, 1428,\n",
       "        1439, 1442, 1443, 1451, 1454, 1465, 1468, 1476, 1485, 1488, 1500,\n",
       "        1510, 1521, 1522, 1535, 1558, 1559, 1562, 1563, 1569, 1570, 1573,\n",
       "        1577, 1590, 1591, 1593, 1596, 1598, 1601, 1603, 1609, 1612, 1613,\n",
       "        1615, 1619, 1624, 1625, 1626, 1635, 1636, 1638, 1652, 1656, 1678,\n",
       "        1680, 1683, 1690, 1691, 1692, 1703, 1706, 1721, 1724, 1725, 1726,\n",
       "        1736, 1739, 1740, 1742, 1759, 1760, 1761, 1762, 1763, 1781, 1783,\n",
       "        1787, 1790, 1792, 1797, 1801, 1816, 1819, 1825, 1831, 1833, 1834,\n",
       "        1837, 1845, 1852, 1853, 1855, 1856, 1859, 1861, 1865, 1867, 1868,\n",
       "        1873, 1878, 1886, 1893, 1898, 1899, 1910, 1926, 1929, 1933, 1949,\n",
       "        1951, 1959, 1965, 1971, 1973, 1974, 1981, 1986, 1990, 1998, 2005,\n",
       "        2007, 2008, 2009, 2021, 2041, 2045, 2047, 2049, 2058, 2062, 2067,\n",
       "        2074, 2076, 2081, 2086, 2089, 2096, 2100, 2112, 2123, 2124, 2135,\n",
       "        2140, 2144, 2146, 2153, 2155, 2162, 2163, 2170, 2172, 2177, 2180,\n",
       "        2186, 2187, 2189, 2194, 2198, 2199, 2202, 2203, 2204, 2208, 2215,\n",
       "        2216, 2220, 2223, 2225, 2226, 2230, 2231, 2235, 2241, 2246, 2249,\n",
       "        2250, 2257, 2258, 2260, 2267, 2269, 2272, 2274, 2277, 2282, 2283,\n",
       "        2285, 2289, 2293, 2295, 2300, 2303, 2304, 2321, 2323, 2325, 2327,\n",
       "        2328, 2329, 2332, 2335, 2348, 2350, 2353, 2361, 2362, 2367, 2368,\n",
       "        2375, 2377, 2378, 2380, 2384, 2387, 2389, 2393, 2408, 2410, 2414,\n",
       "        2420, 2431, 2433, 2438, 2439, 2445, 2451, 2469, 2470, 2482, 2486,\n",
       "        2492, 2497, 2498], dtype=int64),)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_pred_index != y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index 5 \n",
    "- 正解→0(肯定)　予測→1(否定)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_index[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11030001, 0.88970006], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Are You in the House Alone?\" belongs to the pre-cable TV days when the networks were eager to offer an alternative to popular TV shows. It is well-made thriller with a talented cast and credible situations. Kathleen Beller plays a High School student who gets a series of threatening letters. Everyone seems to think that it is nothing more than a prank but Beller is really scared. Tony Bill and Blythe Danner play Beller\\'s parents, Ellen Travolta (John\\'s sister) is the High School Principal and Dennis Quaid has one of his earliest roles as a cocky rich kid. It\\'s a competent chiller with a still relevant social message. Beller is lovely - if you are 30 or older, you will remember that she was very popular among youngsters. Blythe Danner, who I usually don\\'t like, gives a truly moving performance. Nice little film.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 和訳\n",
    "「あなたは一人で家にいますか？」は、ネットワークが人気のあるテレビ番組に代わるものを提供することを熱望していたケーブルテレビ前の時代に属しています。 Tony BillとBlythe DannerがBellerの両親を演じるのは、Ellen Travolta（Johnの姉妹）である。 High School PrincipalとDennis Quaidは、元気いっぱいの金持ちの子供としての彼の初期の役割の1つを持っていますまだ関連性のあるソーシャルメッセージを持つ有能なチラーですBeller is lovely  - あなたは30歳以上の場合、彼女は非常に人気があったことを覚えている 私は普段好きではないBlythe Dannerが本当に感動的なパフォーマンスを提供します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考察\n",
    "- 肯定的な文章（「本当に感動的なパフォーマンスを提供します」等）は文章の最後の方しか記載されていない為、否定的と判断したと思われる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index 18\n",
    "- 正解→1(否定)　予測→0(肯定)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_index[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9397095 , 0.06029051], dtype=float32)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A huge cast gathered for this remake which sadly was a box office failure notwithstanding a great sound track. I can't say it was riveting entertainment, nor a cure for insomnia. Nevertheless I enjoyed the film - it provided the escape I was after one afternoon. A good look for those of us looking for the ideal life, albeit a fantasy. Expect some corny moments, few thrills, and an occasional laugh.\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 和訳\n",
    "このリメイクのために巨大なキャストが集まりました。悲しいことに、素晴らしいサウンドトラックにもかかわらず興行失敗でした。 私はそれが娯楽を惹きつけていた、または不眠症の治療法であるとは言えません。 それにもかかわらず、私はその映画を楽しんだ - それは私が午後の後にあった逃避を提供した。 ファンタジーにもかかわらず、理想的な生活を探している私たちの人々のための良い外観。 いくつかの愚かな瞬間、いくつかのスリル、そして時折笑いを期待"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考察\n",
    "- 否定的な単語より、肯定的な単語(「惹きつけていた」「楽しんだ」等)が多くある為、肯定と判断したと思われる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index 2498\n",
    "- 正解→1(否定)　予測→0(肯定)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[2498]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_index[2498]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6471061, 0.3528939], dtype=float32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[2498]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I could have done with the seven gunslingers just staying away. This sequel should never have been done, the first did it all and better. The plot was a turkey, the acting was turkey, the direction, production, camera work... all turkey. Whoever put out this junk should be tarred and feathered. May they not return again!'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[2498]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 和訳\n",
    "私は7人の砲手たちがただ遠ざかっているだけで終わったかもしれません。 この続編は行われたことがないはずです、最初はそれがすべてよりよくしました。 プロットは七面鳥、演技は七面鳥、演出、演出、カメラワーク…すべて七面鳥だった。 このゴミを出した人は誰でもうんざりして羽毛が付いているはずです。 二度と戻ってこないように！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 考察\n",
    "- 感情的な文章（「二度と戻ってこないように！」等）が多く、各クラスの予測確率が近い値となっている事から、一般的な文章より判断が難しいと思われる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7HMI0Icrsgl"
   },
   "source": [
    "# 【問題8】自然言語処理の応用事例\n",
    "## 目的\n",
    "- NLPの情報共有\n",
    "- 英語の処理の実践\n",
    "\n",
    "現在自然言語処理はどのような企業でどのように活用されているか？ \n",
    "1つ例をあげて3~5分で発表してください。 \n",
    "(例)メルカリは商品説明をTF-IDFを用いてベクトル化して商品の異常検知を行っている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3aL_DZSrsgl"
   },
   "source": [
    "## 調査内容「自然言語処理を駆使しAIが営業活動を分析(ストックマーク株式会社 Asales)」\n",
    "### ①概要\n",
    "- AIが「社内の定性データ(資料、メモ等)データを理解」「顧客・業界動向の収集」を行い、営業活動を分析する\n",
    "\n",
    "### ②何ができるのか？\n",
    "#### 受失注分析\n",
    "- 案件のメモや報告書・メールなどのテキストデータから、受失注の要因を分析。\n",
    "- 4C等のフレームワークをベースに自社の営業活動における強み・弱みを俯瞰。\n",
    "\n",
    "#### 日報・対話解析\n",
    "- 日報や日々のメールを自動解析。\n",
    "- リアルタイムにデータを解析に優先して営業を行うべき案件を自動抽出・提案。\n",
    "- 日報の要約も自動生成し、報告業務もサポート。\n",
    "\n",
    "#### 情報武装\n",
    "- 訪問予定顧客の最新トピックをPush配信し、業界・業種、ソリューション関連情報も網羅。\n",
    "- 社員の情報感度を向上。\n",
    "\n",
    "#### 提案資料作成補助\n",
    "- 過去の類似案件・受注案件の提案資料を自動解析・提案。誰が・いつ・どんな提案をしているかの具体的な特徴を抽出し、提案資料作成を補助。\n",
    "\n",
    "### 補足情報\n",
    "#### 4C分析\n",
    "- 「買う側の視点」に立ってサービスのメリットを整理し、商品の開発や改善のために使われている手法\n",
    "\n",
    "| 4Cの意味                                      |\n",
    "|-----------------------------------------------|\n",
    "| 顧客価値（Customer Value）                  |\n",
    "| 顧客にとっての経費（Cost）                  |\n",
    "| 顧客利便性（Convenience）                   |\n",
    "| 顧客とのコミュニケーション（Communication） |\n",
    "\n",
    "### 参考リンク\n",
    "[Asalesについて](https://stockmark.ai/asales/)\n",
    "\n",
    "[4C分析について](https://hansokunodaigaku.com/keiei_post/3219/)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "S2RZfRiIrsfI",
    "AiM7Qc6lrsfl",
    "S8aCgC7-rsf1",
    "TV6yy2Uursga",
    "nJk_K5SZrsgf",
    "k3aL_DZSrsgl"
   ],
   "name": "sprint22-nlp-intro_0714.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
