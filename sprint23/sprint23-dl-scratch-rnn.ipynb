{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.この課題の目的\n",
    "スクラッチを通してRNNの基礎を理解する\n",
    "以下の要件をすべて満たしていた場合、合格とします。\n",
    "\n",
    "※Jupyter Notebookを使い課題に沿った検証や説明ができている。\n",
    "\n",
    "# 2.スクラッチによる実装\n",
    "NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "Sprint11で作成したディープニューラルネットワークのクラスを拡張する形でRNNを作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "今回はバッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用クラスのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2へのRNN\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features : int\n",
    "        特徴量の数\n",
    "    n_nodes : int\n",
    "        ノード数\n",
    "    activate : インスタンス\n",
    "        最適化を行うクラスのインスタンス\n",
    "    initializer : インスタンス\n",
    "        初期化方法のインスタンス\n",
    "        \n",
    "    Atribute\n",
    "    ----------\n",
    "    self.Wx : numpy配列\n",
    "        入力に対する重み\n",
    "    self.Wh : numpy配列\n",
    "        前の時刻から伝わる順伝播に対する重み\n",
    "    self.B : numpy配列\n",
    "        バイアス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_nodes, activate, initializer):\n",
    "        self.activate = activate\n",
    "        self.initializer = initializer\n",
    "        self.Wx = self.initializer.W(n_features, n_nodes)\n",
    "        self.Wh = self.initializer.W(n_nodes, n_nodes)\n",
    "        self.B = self.initializer.B()\n",
    "\n",
    "        \n",
    "    def forward(self, Xt, forward_Ht=None):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        Xt : numpy配列\n",
    "            入力データ\n",
    "        forward_Ht : numpy配列\n",
    "            時刻t-1の状態（前の時刻から伝わる順伝播）\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        Ht : numpy配列\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        # 前の時刻から伝わる順伝播の有無を確認\n",
    "        if forward_Ht is not None:\n",
    "            # あり\n",
    "            At = Xt @ self.Wx + forward_Ht @ self.Wh + self.B\n",
    "        else:\n",
    "            # なし\n",
    "            At = Xt @ self.Wx + self.B\n",
    "        \n",
    "        # 活性化関数クラスのインスタンスで順伝搬\n",
    "        Ht = self.activate.forward(At)\n",
    "        \n",
    "        return Ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanhクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Tanhの計算を実施\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : numpy配列\n",
    "            入力データ\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        tanh_A: numpy配列\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        # 計算結果を格納しreturnする\n",
    "        tanh_A = np.tanh(A)\n",
    "\n",
    "        return tanh_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleInitializerクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer():\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    self.sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sigma):\n",
    "\n",
    "        self.sigma = sigma\n",
    "\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          作成する配列の行数\n",
    "        n_nodes2 : int\n",
    "          作成する配列の列数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "        return W\n",
    "    \n",
    "\n",
    "    def B(self):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1)\n",
    "\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。\n",
    "\n",
    "```python\n",
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes))\n",
    "b = np.array([1])\n",
    "```\n",
    "フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。\n",
    "```python\n",
    "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題2では重みとバイアスが指定されている為、以下の専用のクラスを使用しフォワードプロパゲーションの動作確認を行う\n",
    "\n",
    "## Test_SimpleRNNクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_SimpleRNN:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2へのRNN\n",
    "    Parameters\n",
    "    ----------\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "\n",
    "    batch_size : バッチサイズ7\n",
    "    n_features : 入力特徴量\n",
    "    n_nodes : RNNのノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, Wx, Wh, B, activate, initializer=None):\n",
    "        self.activate = activate\n",
    "        self.initializer = initializer\n",
    "        self.Wx = Wx\n",
    "        self.Wh = Wh\n",
    "        self.B = B\n",
    "        \n",
    "        \n",
    "    def forward(self, Xt, forward_Ht=None):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        if forward_Ht is not None:\n",
    "            At = Xt @ self.Wx + forward_Ht @ self.Wh + self.B\n",
    "        else:\n",
    "            At = Xt @ self.Wx + self.B\n",
    "        \n",
    "        Ht = self.activate.forward(At)\n",
    "        \n",
    "        return Ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初期値設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes))\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forwardの動作確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for文を使用した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN = Test_SimpleRNN(Wx=w_x, Wh=w_h, B=b, activate=Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN_out = [0.79494228 0.81839002 0.83939649 0.85584174]\n"
     ]
    }
   ],
   "source": [
    "for sequence in range(x.shape[1]):\n",
    "    \n",
    "    # 時刻0の時は前の時刻から伝わる順伝播がない\n",
    "    if sequence == 0:\n",
    "        # 時刻0→前の時刻から伝わる順伝播がない場合の計算\n",
    "        RNN_out = RNN.forward(x[0][sequence], None)\n",
    "    else:\n",
    "        # 時刻0以外→前の時刻から伝わる順伝播がある場合の計算\n",
    "        RNN_out = RNN.forward(x[0][sequence], RNN_out)\n",
    "        \n",
    "print('RNN_out = {}'.format(RNN_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インスタンスを複数作成した場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN1 = Test_SimpleRNN(Wx=w_x, Wh=w_h, B=b, activate=Tanh())\n",
    "RNN2 = Test_SimpleRNN(Wx=w_x, Wh=w_h, B=b, activate=Tanh())\n",
    "RNN3 = Test_SimpleRNN(Wx=w_x, Wh=w_h, B=b, activate=Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN3_out = [0.79494228 0.81839002 0.83939649 0.85584174]\n"
     ]
    }
   ],
   "source": [
    "RNN1_out = RNN1.forward(x[0][0], None)\n",
    "RNN2_out = RNN2.forward(x[0][1], RNN1_out)\n",
    "RNN3_out = RNN3.forward(x[0][2], RNN2_out)\n",
    "print('RNN3_out = {}'.format(RNN3_out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
